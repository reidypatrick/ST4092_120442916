---
title: "Modelling Architectures Underlying Financial Pricing Problems"

date: "`r Sys.Date()`"

author: "Patrick Reidy"

affiliation: "School of Mathematical Sciences, University College Cork"

email: "120442916@umail.ucc.ie"

output: 
  bookdown::pdf_document2:
    fig_crop: no
    keep_tex: FALSE
    toc: FALSE
    extra_dependencies: ["subfig", "tikz", "float"]
  author:
  - name: "Patrick Reidy"
    affiliation: "School of Mathematical Sciences, University College Cork"
    email: "120442916@umail.ucc.ie"
    
header-includes:
  - \usepackage{titling}
  - \pretitle{\begin{center}
    \includegraphics[width=4in,height=2in]{data/figures/University_College_Cork_logo.png}\LARGE\\}
  - \posttitle{\end{center}}
  - \usepackage{tikz}
  - \usetikzlibrary{positioning, arrows.meta, shapes.geometric}
  - \usepackage{amsmath}
  


editor_options: 
  chunk_output_type: inline
  
bibliography: references/fyp_references.bib
csl: https://www.zotero.org/styles/ieee
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(paged.print = TRUE)
knitr::opts_chunk$set(fig.path = "data/figures/")
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")

library(OpenML)
library(tidyverse)
library(tidymodels)
library(raster)
library(ggplot2)
library(gridExtra)
library(knitr)
library(kableExtra)
library(keras)

source("R/functions/source_functions.R", verbose = FALSE)
source_functions()

# config
options(log_verbose = FALSE)
options(source_or_run = "source")

cars_orig <- OpenML::getOMLDataSet(data.id = 41214)$data
```

\newpage

\tableofcontents

\newpage

\listoffigures

\newpage

\listoftables

\newpage

# Glossary {.unnumbered}

GLM

:   Generalised Linear Model

NN

:   Neural Network

FFNN

:   Feed Forward Neural Network

LSE

:   Least Squares Estimator

MLE

:   Maximum Likelihood Estimator

EDF

:   Exponential Distribution Family

\newpage

# Abstract

The approach to insurance pricing has witnessed a significant evolution in modelling techniques over the years, with a transition from traditional regression methods to more sophisticated models such as Generalized Linear Models (GLMs) and, most recently, Deep Learning (DL) models.

Classical linear regression is the earliest and simplest form of modelling and has been used in a variety of predictive applications due to its ease of interpretation and implementation, it is not however appropriate in the context of insurance pricing as it is not designed to handle the complexities of higher-order applications, such as the representation of non-linear relationships. This is particularly problematic in the context of insurance data, where the outcome is heavily influenced by complex interactions between multiple fields and attributes. Consequently, Generalized Linear Models have emerged since as a significant advancement, offering a more flexible framework that accommodated various distribution families and data types. GLMs excel in modelling discrete and continuous outcomes while retaining interpretability, making them the go-to choice for many years.

Deep Learning models have recently taken industries across the world by storm, not least the insurance industry, ushering in a new era of predictive accuracy. Neural networks, with their multilayered architecture, can capture intricate patterns in massive datasets, including high-dimensional data, unstructured data, and temporal data. DL models have demonstrated remarkable capabilities in feature extraction, non-linear relationship modelling, and predictive accuracy. They are capable of handling diverse data types such as images, text, and tabular data, which is particularly valuable in the age of big data. These advantages extend to the world of insurance pricing, where their potential remains yet to be completely realised. *TODO: expand!*

The transition from GLMs to Deep Learning is not without its challenges. While DL models excel in predictive performance, they often sacrifice interpretability, which is a crucial requirement in the insurance industry for regulatory compliance and trustworthiness. Additionally, the capabilities of unsupervised learning has been at times overestimated, with many foregoing vital data preprocessing and investigation in the excitement of early demonstrations of these models' performance *TODO: expand!*. Ensuring model fairness *??, expand* and avoiding bias in DL models remains an active area of research, as it is crucial for ethical underwriting and legal compliance. One method to address these concerns is to improve the explainability of these models, or bridge the gap in performance between traditional GLMs and DL models.

\clearpage

# Introduction

Insurance pricing is the task of calculating the optimal price offered by an insurance provider to a potential policyholder. In order to determine the price, it is necessary to determine first the risk posed by the person to be insured. This risk is evaluated using statistical models that use factors related to the risk. For example, in the context of car insurance, the profiles of both the driver and the car impact how risk is considered and priced.

Insurance providers are constantly searching for ways to improve and gauge the performance of these models. We will discuss several of these models, see how they are related and discuss ongoing developments in the insurance pricing industry.

To understand these models, we will first begin with the simplest form of modelling, namely simple linear regression. However, in the case of car insurance, linear regression is not appropriate, as in its simplicity it can not reflect the spread of our data, nor the complex non-linear relationships between the selected risk factors and insurance claims. We will therefore introduce two concepts, namely Maximum Likelihood Estimation and the Exponential Distribution Family, to see how this model can be expanded to a more versatile family of models called Generalised Linear Models. This will yield a great deal of information as to the risk associated with an insurance profile. Furthermore, these models will then be again extended and generalised to produce an even more powerful modelling architecture: Neural Networks.

Neural Networks often trade off the ease with which GLMs are interpreted for greater predictive accuracy, i.e. calculating the risk more precisely, but yielding little information as to what factors specifically contribute to this risk. It will be discussed that through careful consideration of our NN architecture, one can indeed extract valuable information regarding the relationship between factors and risk.

## Simple Linear Regression {.unnumbered}

Simple linear regression is a method of modelling the linear relationship between two random variables, say $X = (x_1,...,x_i,...x_n)$ and $Y = (y_1, ..., y_i, ...y_n)$. We represent this relationship as a line where

$$
\hat{Y} = \beta_0 + \beta_1X \\
$$

Or for each observation

$$
\hat{y_i} = \beta_0 + \beta_1x_i
$$

Where $\hat{Y} = (\hat{y_1}, ..., \hat{y_n})$ are our predicted $y_i$ values based on the corresponding $x_i$ and $\beta_0$ is the intercept term, and $\beta_1$ is the slope coefficient.

```{r figLMLSE, include=TRUE, echo=FALSE, fig.cap="Linear Model: LSE", paged.print = TRUE, out.height="50%"}
# Set seed for reproducibility
set.seed(123)

# Generate nearly linear data
x <- 1:25
y_true <- 2 * x + rnorm(25, mean = 0, sd = 5)
# True linear relationship with normally distributed error

# Fit linear model
model <- lm(y_true ~ x)

# Predictions from the model
y_pred <- predict(model)

# Plot the data and linear model
plot(
  x,
  y_true,
  col = "blue",
  pch = 20,
  xlab = "X",
  ylab = "Y"
)
abline(model, col = "red")

# Add lines indicating normally distributed errors
segments(x, y_true, x, y_pred, col = "green")

# Add legend
legend("topleft",
  legend = c(
    expression(y[i]), expression(y[i] - hat(y)[i]),
    expression(hat(Y) == beta[0] + beta[1] * X)
  ),
  col = c("blue", "green", "red"),
  pch = c(20, NA, NA),
  lty = c(NA, 1, 1),
  cex = 0.8
)
```

Above we can see a plot where the blue dots are the observations and the red line is our $\hat{Y} = \beta_0+\beta_1X$. Also included are the green lines, which represent the difference between the prediction and the true value, $y_i-\hat{y_i}$. In order to find the best fitting line, we get the sum of the squares of these differences and minimise this value, yielding the Least Squares Estimate (LSE) of our regression parameters $\beta_0$, $\beta_1$:

$$
LSE(\beta) = \min\sum(y_i-\hat{y}_i)^2
$$

Recalling that

$$\hat{y_i} = \beta_0 + \beta_1x_i$$ We can write this as

```{=tex}
\begin{equation} LSE(\beta) = \min\sum(y_i-(\beta_0+\beta_1x_i))^2 (\#eq:LSE) \end{equation}
```
Now we can find, using partial derivatives, the $\beta_0$ and $\beta_1$ that minimise this value.

This can also be extended for Multiple Linear Regression, where we have more than one predictor variable $x$, and instead we use the formula

$$
\hat{y_i} = \beta_0 + \sum^p_{j=1}\beta_jx_{ij}
$$

Where i = 1,...,n.

However, for the time being, it will be assumed there is only one predictor variable $x$ for ease of notation.

## Maximum Likelihood Estimator {.unnumbered}

A different approach lies in understanding that often we can assume the following:

1.  The observations $y_i$ are independent

2.  The observations are normally distributed with $y_i \sim N(\mu_i, \sigma^2)$

3.  We can estimate these $\mu_i$ with a line of the form $\mu_i = \beta_0 + \beta_1X_i$

```{r figLMMLE, fig.cap = "Linear Model: MLE", , out.height="50%"}
# Set seed for reproducibility
set.seed(123)

# Generate nearly linear data
x <- 1:25
y_true <- 2 * x + rnorm(25, mean = 0, sd = 3)
# True linear relationship with normally distributed error

# Fit linear model
model <- lm(y_true ~ x)

# Predictions from the model
y_pred <- predict(model)

# Plot the data and linear model
plot(
  x,
  y_true,
  col = "blue",
  pch = 20,
  main = "Linear Model: MLE",
  xlab = "X",
  ylab = "Y"
)
abline(model, col = "red")

# Add bell curves around each true data
# point to represent normally distributed errors
for (i in seq(1, 25, 5)) {
  points <- rnorm(100, mean = y_pred[i], sd = 1.5)
  d <- density(points)
  lines(i + d$y, d$x, col = "green", lwd = 2)
}

legend(
  "topleft",
  legend = c(
    expression(y[i]),
    expression(
      paste("f(y"[i], " | y"[i], " ~ N(", mu[i], ", ", sigma^2, "))")
    ),
    expression(hat(Y) == beta[0] + beta[1] * X)
  ),
  col = c("blue", "green", "red"),
  pch = c(20, NA, NA),
  lty = c(NA, 1, 1),
  cex = 0.8
)
```

Above is a representation of these assumptions, where we still see some linear relationship between X and Y, where the green lines represent the density of each distribution $N(\mu_i, \sigma^2)$. The key aspect here is we are no longer fitting a line to the observations Y, rather to the means of the normal distributions from which they come. We can then evaluate the likelihood of the ith observation being from the ith distribution with the pdf

$$
\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y_i - \mu_i)^2}{2\sigma^2}}
$$

To find the joint probability across all observations we get

$$
\prod^n_{i=1} \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y_i - \mu_i)^2}{2\sigma^2}}
$$

But recall that $\mu_i = \beta_0+\beta_1x_i$, and we would like to maximise this probability with respect to the $\beta's$. We can write this as instead a likelihood function

$$
\mathcal{L}(\beta|Y) = \prod^n_{i=1} \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y_i - \mu_i)^2}{2\sigma^2}}
$$

We know that any $\beta$ that maximises this function also maximises the log likelihood function denoted $\ell(\beta|Y)$, which also helps in evaluating the right hand side

$$
\ell(\beta|Y) = \sum^n_{i=1} -\log(\sqrt{2\pi\sigma^2}) - \frac{(y_i - \mu_i)^2}{2\sigma^2}
$$

Seeing as we are only interested in terms here that relate to $\beta$ and $y_i$, we can ignore some terms and finally we have

$$
\ell(\beta|Y) = -\sum(y_i-\mu_i)^2
$$

We now want to maximise this value, but as it is a negative quantity, this is the same as minimising the negative of the function, and notate this as our Maximum Likelihood Estimator (MLE)

$$
MLE(\beta) = \min\sum(y_i-\mu_i)^2
$$

If we replace $\mu_i$ with our linear model, we get

```{=tex}
\begin{equation} MLE(\beta) = \min\sum(y_i-(\beta_0 + \beta_1x_i)^2 (\#eq:MLE) \end{equation}
```
Note: Once again, this form can be extended for multiple predictor variables $x$, where it takes the form

$$ MLE(\beta) = \min\sum^n_{i=1}(y_i-(\beta_0 + \sum^p_{j=1}\beta_jx_{ij}))^2 $$

In the case of a normally distributed random variable, the MLE \@ref(eq:MLE) is identical to the LSE \@ref(eq:LSE). However, for non-normal variables, we require a different framework which first requires addressing the normal distribution deeper, particularly the family of distributions, the Exponential Distribution Family (EDF).

## Exponential Distribution Family {.unnumbered}

Nelder and Wedderburn @nelder_1972_generalized devised a method for generalising the above form, by first generalising the pdf and the notion of a family of distributions. This family takes the form

```{=tex}
\begin{equation} f(y) = exp\left(\frac{y\theta - b(\theta)}{a(\phi)} + c(y, \phi)\right) (\#eq:EDF) \end{equation}
```
Where:

-   $y$ is some observation from a distribution in the EDF

-   $\theta$ is called the natural or canonical parameter, that itself can be expressed as some function of the true mean as $g(\mu)$

-   $\phi$ is called the dispersion parameter

-   $a$, $b$ and $c$ are some known functions of these parameters

It is further shown that by treating the above as a likelihood, and maximising with respect to $\theta$

$$
\frac{\partial\mathcal{L}}{\partial\theta} = y-b'(\theta) := 0
$$

Where $b'(\theta)$ will be equal to the true mean of this distribution, so

$$
\frac{\partial\mathcal{L}}{\partial\theta} = y-\mu = 0
$$

In other words, finding some optimal predictor for $\theta$ will effectively obtain the best estimate for the true mean of the distribution from which $y$ comes.

For the normal distribution we can see that we can take

$$
f(y) = \frac{1}{\sqrt{2\pi\sigma^2}}exp\left({\frac{(y-\mu)^2}{2\sigma^2}}\right)
$$

and rewrite this as

```{=tex}
\begin{align*} 
f(y) & = exp\left( -\frac{(y^2 + \mu^2 - 2y\mu)}{2\sigma_2} -\frac{1}{2}log(2\pi\sigma^2)\right) \\ 
f(y) & = exp\left(\frac{y\mu - \frac{\mu^2}{2}}{\sigma^2} - \frac{y^2}{2\sigma^2} -
\frac{1}{2}log(2\pi\sigma^2)   \right)
\end{align*}
```
And so to obtain \@ref(eq:EDF) we choose the following

```{=tex}
\begin{align*}
\theta & = \mu &
b(\theta) & = \frac{\theta^2}{2}  = \frac{\mu^2}{2} \\
\phi & = \sigma^2 & 
a(\phi) & = \phi  =\sigma^2 \\
c(y,\phi) & = - \frac{y^2}{2\sigma^2} -
\frac{1}{2}log(2\pi\sigma^2)
\end{align*}
```
Here, our natural parameter $\theta$ is simply equal to the mean $\mu$, with $g(\theta)$ simply being the identity function. It is also demonstrated here that

$$
b'(\theta) = \frac{d}{d\theta}\frac{\theta^2}{2} = \theta = \mu
$$

With the combined notions of the MLE and the EDF, we can generalise this process to work for any distribution in the EDF.

## Generalised Linear Models {.unnumbered}

```{r figGLMMLE, fig.cap = "Non-linear Model"}
set.seed(09022024)
x <- 1:25
y_true <- x * rexp(25, rate = 1)
# True linear relationship with normally distributed error

# Fit linear model
model <- lm(y_true ~ x)

# Predictions from the model
y_pred <- predict(model)


# Plot the data and linear model
plot(
  x,
  y_true,
  col = "blue",
  pch = 20,
  xlab = "X",
  ylab = "Y",
  ylim = c(0, 85)
)
abline(model, col = "red")

for (i in seq(2, 25, 4)) {
  points <- rexp(10000, rate = 0.5)
  d <- density(points)
  lines(i + d$y, y_true[i] + d$x, col = "green", lwd = 2)
}

legend(
  "topleft",
  legend = c(
    expression(y[i]),
    expression(paste("f(y"[i], ")")),
    expression(hat(Y) == beta[0] + beta[1] * X)
  ),
  col = c("blue", "green", "red"),
  pch = c(20, NA, NA),
  lty = c(NA, 1, 1),
  cex = 0.8
)
```

Here we see that our $y_is$ are clearly not distributed normally, rather by some other distribution $f(y_i)$ illustrated by the green densities, and as such any linear predictor, such as is shown in red, will not accurately model our data.

For the purpose of this report, we will be looking at insurance claim as a count, which is known to follow a Poisson distribution, the density of which can be seen below.

```{r figPoiHist, fig.cap = "Histogram of a Poisson Random Variable"}
set.seed(120442916)
y <- rpois(100, 4)

hist(y, freq = FALSE)
lines(density(y), lwd = 2, col = "green")
```

For the poisson distribution, we begin with

$$ f(y) = \frac{e^{-\lambda}\lambda^y}{y!} $$

This can be written as

```{=tex}
\begin{align*} f(y) & = exp\left(-\lambda + y\log(\lambda) -\log(y!) \right) \tag{**} \\ f(y) & = exp\left( \frac{y\log(\lambda) - \lambda}{1} -\log(y_i!)\right)  \end{align*}
```
And so to obtain \@ref(eq:EDF) we choose

```{=tex}
\begin{align*} \theta & = \log(\lambda) & b(\theta) = e^\theta = \lambda \\ \phi & = 1 &  a(\phi) = 1 \\ c(y,\phi) & = -\log(y!) \end{align*}
```
And in this case our natural parameter $\theta$ is equal to the log of our true mean $\lambda$. From this we conclude that to best predict the true mean of the distribution of some $y_i \sim Poi(\lambda_i)$, we now fit a model to this function of $\lambda_i$.

$$ \log(\lambda_i) = \beta_0 + \beta_1x_i $$

Or for multiple $x$

```{=tex}
\begin{equation} \log(\lambda_i) = \beta_0 + \sum^p_{j=1}\beta_jx_{ij} (\#eq:PoiGLM) \end{equation}
```
In the terminology of @nelder_1972_generalized, a GLM is made up of two components: The random component and the systematic component. The random component refers to $\beta_0 + \Sigma^p_{i=1}\beta_ix_i$, that is the linear combination of our predictor variables and our regression parameters, arrived at using the MLE. The random component refers to the $g(\mu_i)$, that is our link function, which captures the fact that our data emerges from a probability distribution of some mean $\mu$ which is dependent on our predictor variables $x_1, \ldots, x_p$ . This yields our general formula for a GLM

```{=tex}
\begin{equation} g(\mu_i) = \beta_0 + \sum^p_{j=1}\beta_jx_{ij} (\#eq:GLM) \end{equation}
```
## Neural Networks

### Structure

Neural Networks (NNs), while possibly theorised in the mid 20th century, only began realising their potential in recent decades, as computational resources began to enable the architecture to outperform classical statistical models. As discussed in @lu2020universal, a two layer "deep" NN is sufficient in the context of most problems. The architecture for such networks can be seen below.

```{r figIntroNetwork, results="asis", fig.cap = "Architecture of Neural Network"}
cat(get_network_tex(
  "figIntroNetwork",
  n_input = "p",
  n_hidden1 = "q",
  n_hidden2 = "q"
))
```

```{=tex}
\begin{figure}
  \centering
    \begin{tikzpicture}[>=stealth, node distance=1.5cm, every node/.style={scale=0.8}]
    % Input layer
    \node[circle, draw, fill=yellow!20] (X) at (0,-1) {$X$};
    \node[circle, draw, fill=blue!20] (W1) at (0,-2) {$W_1$};
    \node[circle, draw, fill=blue!20] (b1) at (0,-3) {$b_1$};

    \node[circle, draw, fill=green!20] (star1) at (1,-1) {$*$};
    \node[circle, draw, fill=green!20] (plus1) at (2,-1) {$+$};
    
    \node[circle, draw, fill=red!20] (act_fun1) at (3,-1) {$ Act1 $};

    \node[circle, draw, fill=yellow!20] (h1) at (4,-1) {$H_1$};

    % Labels
    \node[above=0.5cm of X] {Input Layer};

    % Connect Nodes
    \draw[->] (X) -- (star1);
    \draw[->] (W1) -- (star1);

    \draw[->] (star1) -- (plus1);
    \draw[->] (b1) -- (plus1);

    \draw[->] (plus1) -- (act_fun1);
    \draw[->] (act_fun1) -- (h1);

    % First Hidden Layer
    \node[circle, draw, fill=blue!20] (W2) at (4,-2) {$W_2$};
    \node[circle, draw, fill=blue!20] (b2) at (4,-3) {$b_2$};

    \node[circle, draw, fill=green!20] (star2) at (5,-1) {$*$};
    \node[circle, draw, fill=green!20] (plus2) at (6,-1) {$+$};
    
    \node[circle, draw, fill=red!20] (act_fun2) at (7,-1) {$ Act2 $};

    \node[circle, draw, fill=yellow!20] (h2) at (8,-1) {$H_2$};

    % Labels
    \node[above=0.5cm of h1] {Hidden Layer 1};

    % Connect Nodes
    \draw[->] (h1) -- (star2);
    \draw[->] (W2) -- (star2);

    \draw[->] (star2) -- (plus2);
    \draw[->] (b2) -- (plus2);

    \draw[->] (plus2) -- (act_fun2);

    % Second Hidden Layer
    \node[circle, draw, fill=blue!20] (W3) at (8,-2) {$W_3$};
    \node[circle, draw, fill=blue!20] (b3) at (8,-3) {$b_3$};

    \node[circle, draw, fill=green!20] (star3) at (9,-1) {$*$};
    \node[circle, draw, fill=green!20] (plus3) at (10,-1) {$+$};

    \node[circle, draw, fill=red!20] (act_fun3) at (11,-1) {$ Act3 $};

    % Labels
    \node[above=0.5cm of h2] {Hidden Layer 1};

    % Connect Nodes
    \draw[->] (act_fun2) -- (h2);
    \draw[->] (h2) -- (star3);
    \draw[->] (W3) -- (star3);

    \draw[->] (star3) -- (plus3);
    \draw[->] (b3) -- (plus3);

    \draw[->] (plus3) -- (act_fun3);

    % Final Layer
    \node[circle, draw, fill=yellow!20] (output) at (12,-1) {$\hat{y_i}$};
    \node[circle, draw, fill=red!20] (loss) at (12,-2) {$Loss$};
    \node[circle, draw, fill=yellow!20] (truth) at (12,-3) {$y_i$};

    % Labels
    \node[above=0.5cm of act_fun3] {Output};

    % Connect Nodes
    \draw[->] (act_fun3) -- (output);
    \draw[->] (output) -- (loss);
    \draw[->] (truth) -- (loss);

  \end{tikzpicture}
  \caption{Computation Graph of Neural Network}
  \label{fig:figIntroDummyCompGraph}
\end{figure}
```
Figure \ref{fig: figIntroNetwork } and Figure \ref{fig:figIntroDummyCompGraph} are two helpful visualisations of NNs. Figure \ref{fig: figIntroNetwork } shows the fully connected network, with two hidden layers. Figure \ref{fig:figIntroDummyCompGraph} shows the computation graph where:

-   $X$ is a vector of predictor variables

-   The blue nodes represent the model parameters, the weights $W$ and biases $b$, that make up each hidden layer. These are the trainable parameters, equivalent to $\beta_0,...,\beta_p$ in a GLM.

-   The green nodes are simple matrix operations where an incoming arrow indicates input parameters and the outgoing arrows indicate output.

-   The red nodes are our activation functions and also the loss function. The activation functions account for non-linearity in the model, and can be considered similar to how a link function is used in a GLM. The Loss function is used to determine updates to our trainable parameters, similar to how the MLE is used in a GLM.

-   The final two yellow nodes are our prediction $\hat{y_i}$ and the ground truth $y_i$, which are used to calculate the loss function.

It will eventually be seen how we arrive at this model, but firstly, we consider a NN at its simplest. We define a NN with 0 hidden layers, and some activation function $f$. Visualised in a similar fashion to Figure \@ref(fig:figIntroDummyCompGraph) we get

```{=tex}
\begin{figure}
  \centering
    \begin{tikzpicture}[>=stealth, node distance=1.5cm, every node/.style={scale=0.8}]
    % Input layer
    \node[circle, draw, fill=yellow!20] (X) at (0,-1) {$X$};
    \node[circle, draw, fill=blue!20] (W1) at (0,-2) {$W_1$};
    \node[circle, draw, fill=blue!20] (b1) at (0,-3) {$b_1$};

    \node[circle, draw, fill=green!20] (star1) at (1,-1) {$*$};
    \node[circle, draw, fill=green!20] (plus1) at (2,-1) {$+$};
    
    \node[circle, draw, fill=red!20] (act_fun1) at (3,-1) {$ f $};

    % Labels
    \node[above=0.5cm of X] {Input Layer};

    % Connect Nodes
    \draw[->] (X) -- (star1);
    \draw[->] (W1) -- (star1);

    \draw[->] (star1) -- (plus1);
    \draw[->] (b1) -- (plus1);

    \draw[->] (plus1) -- (act_fun1);

    

    % Final Layer
    \node[circle, draw, fill=yellow!20] (output) at (4,-1) {$\hat{y_i}$};
    \node[circle, draw, fill=red!20] (loss) at (4,-2) {$Loss$};
    \node[circle, draw, fill=yellow!20] (truth) at (4,-3) {$y_i$};

    % Labels
    \node[above=0.5cm of output] {Output};

    % Connect Nodes
    \draw[->] (act_fun1) -- (output);
    \draw[->] (output) -- (loss);
    \draw[->] (truth) -- (loss);

  \end{tikzpicture}
  \caption{Computation Graph of Zero-Layer NN}
  \label{fig:figZeroLayerCompGraph}
\end{figure}
```
This is now in a form simple enough to express mathematically as below

```{=tex}
\begin{equation}
\hat{y_i} = f \left( \begin{bmatrix} W_1, \ldots, W_p\end{bmatrix} \begin{bmatrix} x_{i1} \\ \vdots \\ x_{ip} \end{bmatrix} + b \right)
\end{equation}
```
We can multiply out the weights with our predictor variables, and move our activation function to the left-hand side, giving

```{=tex}
\begin{equation} f^{-1}(\hat{y_i}) = b + \sum^p_{j=1}W_jx_{ij} (\#eq:ZeroNN) \end{equation}
```
And recalling our general form of our GLM \@ref(eq:GLM)

```{=tex}
\begin{equation} g(\mu_i) = \beta_0 + \sum^p_{j=1}\beta_jx_{ij} \tag{\ref{eq:GLM}} \end{equation}
```
It can be concluded that our simple NN contains both components of a GLM, namely the random component in the form of $g(\mu_i) = f^{-1}(y_i)$, or the inverse of our activation function, and the systematic component $\beta_0 + \sum^p_{j=1}\beta_jx_{ij} = b + \sum^p_{j=1}W_jx_{ij}$. It remains however to discuss in detail how the trainable parameters in NNs are calculated.

### Loss Function

When dealing in Neural Networks, a loss function is used to evaluate the error in the model iteration, which is then minimised by propagating this error backwards through the network, calculating the gradient of the error with respect to each of the weights, and then updating the weights accordingly.

Many different loss functions are available to choose from. In the context of insurance pricing, we believe that given our data are non-negative, discrete counts and therefore follow a poisson distribution. The loss function in this case is obtained identically to the Maximum Likelihood Estimate for the GLM where we have

$$ \ell(\lambda|y) = \sum^n_{i=1} y_ilog(\lambda_i) -\lambda_i + log(y_i!)$$

In this case where $\lambda$ is the latest model output. However instead of maximising this function we want to minimise it, which is equivalent to maximising the negative of this function.

$Loss = \lambda - ylog(\lambda) - log(y!)$

Thus it can be concluded that the systematic component of a GLM is calculated in the same way as that of the trainable parameters of a NN.

### Conclusion

If we recall \@ref(eq:GLM) our GLM formula of

$$\log(\lambda_i) = \beta_0 + \sum^p_{j=1}\beta_jx_{ij}$$

We can rewrite this in matrix notation as

$$ log(\lambda_i) = \beta_0 +  \begin{bmatrix} \beta_1, \ldots, \beta_p\end{bmatrix} \begin{bmatrix} x_{i1} \\ \vdots \\ x_{ip}\end{bmatrix} $$

We can rearrange this as:

$$ log(\lambda_i) = \begin{bmatrix} \beta_1, \ldots, \beta_p\end{bmatrix} \begin{bmatrix} x_{i1} \\ \vdots \\ x_{ip}\end{bmatrix} + \beta_0 $$

And we consider this not as a linear model fitted to a function of some mean, but rather as the exponential of some linear function fitted to the mean we get

```{=tex}
\begin{equation}
\lambda_i = exp \left( \begin{bmatrix} \beta_1, \ldots, \beta_p\end{bmatrix} \begin{bmatrix} x_{i1} \\ \vdots \\ x_{ip} \end{bmatrix} + \beta_0 \right) 
(\#eq:GLMNN)
\end{equation}
```
Now recall our NN formula \@ref(eq:ZeroNN), except with no hidden layers we arrive at

```{=tex}
\begin{equation}
\hat{y} = ActivationFunction \left( \begin{bmatrix} W_1, \ldots, W_p\end{bmatrix} \begin{bmatrix} x_{i1} \\ \vdots \\ x_{ip} \end{bmatrix} + b \right) (\#eq:NNGLM)
\end{equation}
```
We see that designing a NN of 0 hidden layers, with exponential activation function will produce a function \@ref(eq:NNGLM) equivalent to that of a Poisson GLM \@ref(eq:GLMNN). When we also consider that our loss function was defined to be the MLE of a poisson distribution, we see also that the weights and bias of our NN our calculated the same way that a GLM calculates the regression parameters $\beta$.

```{=tex}
\begin{equation} \begin{bmatrix} W_{11} & \ldots & W_{1p} \\                      \vdots & \ddots & \vdots \\                     W_{q1} & \ldots & W_{qp}\end{bmatrix}    \begin{bmatrix} X_1 \\ \vdots \\ X_p\end{bmatrix}  + \begin{bmatrix}b_1 \\ \vdots \\ b_p\end{bmatrix}
 = \begin{bmatrix} H_1 \\ \vdots \\ H_q \end{bmatrix} (\#eq:NNLayer1) \end{equation}
```
# Methodology

```{=html}
<!--
"The data used for this study were collected by..."
-->
```
## Data Collection

This study is carried out using the French motor third partly liability claims frequency data (FreMTPL2freq) available through CASdatasets [@CASdatasets].

```{r tabHead, echo=FALSE, include=TRUE, paged.print=TRUE}
head(cars_orig) %>%
  kable(
    format = "latex", booktabs = TRUE,
    caption = "Structure of the FreMTPL2freq Dataset",
    row.names = FALSE
  ) %>%
  kable_styling(latex_options = "scale_down") %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_styling(latex_options = "striped")
```

The description of the data as outlined in the documentation is as follows:

*"Context: In the dataset freMTPL2freq risk features and claim numbers were collected for 677,991 motor third-part liability policies (observed on a year). Content freMTPL2freq contains 11 columns*

-   *IDpol: The policy ID (used to link with the claims dataset).*

-   *ClaimNb: Number of claims during the exposure period.* This is the variable to be predicted

-   *Exposure: The exposure period.*

-   *Area: The area code.* Six distinct area codes from A to F.

-   *VehPower: The power of the car (ordered categorical).*

-   *VehAge: The vehicle age, in years.*

-   *DrivAge: The driver age, in years (in France, people can drive a car at 18).*

-   *BonusMalus: Bonus/malus, between 50 and 350: 100 means malus in France.*

-   *VehBrand: The car brand (unknown categories).*

-   *VehGas: The car gas, Diesel or regular.*

-   *Density: The density of inhabitants (number of inhabitants per km2) in the city in which the driver of the car lives.*

-   *Region: The policy regions in France (based on a standard French classification) "*

```{r tableSum, echo=FALSE, include=TRUE, paged.print=TRUE}
cars_orig %>%
  mutate(VehGas = as.factor(VehGas)) %>%
  dplyr::select(-IDpol, -where(is.factor)) %>%
  summary() %>%
  knitr::kable(
    format = "latex", booktabs = TRUE,
    caption = "Summary of Numeric Variables in Data"
  ) %>%
  kable_styling(latex_options = "scale_down") %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_styling(latex_options = "striped")
```

## Data Analysis

```{r setup_fig, echo=FALSE}
wd <- file.path("C:/Users/patos/Documents/ST4092_120442916")
dsn <- file.path(wd, "data/input/shapefile")
shape <- shapefile(file.path(dsn, "FRA_adm1.shp"))
```

Before investigating the performance of the models, we carry out a number of investigations regarding the structure and behaviour of the data. This is important in identifying which methods to use, issues to address and results to expect.

*Should first step be investigating e.g. cor matrix, i.e. dataset as a whole?*

As ClaimNb is the response variable it is no doubt the most important variable to understand and the first variable we investigate. Figure \@ref(fig:fig_cor)

*Discuss each plot in turn and insights yielded, Motivation behind each plot*

*Histogram of every factor vs ClaimNb - Area, Vehpower, VehAge, VehBrand, VehGas, Region* *Scatterplot of every numeric vs ClaimNb - Exposure, DrivAge, Density* *Map of every variable*

\newpage

### ClaimNb {.unnumbered .unlisted}

```{r figClaimNb, out.width="33%", out.ncol = 3, fig.cap="ClaimNb", fig.subcap = c("ClaimNb by Region", "Histogram of ClaimNb", "Boxplot of ClaimNb")}
create_map(cars_orig, ClaimNb, shape, source_or_run = "run")
hist(cars_orig$ClaimNb, main = "", xlab = "")
boxplot(cars_orig$ClaimNb)
```

### Exposure {.unnumbered .unlisted}

```{r figExposure, out.width = "33%", out.ncol = 3, fig.cap = "Exposure", fig.subcap=c("Exposure by Region", "Histogram of Exposure", "Boxplot of Exposure")}
create_map(cars_orig, Exposure, shape, source_or_run = "run")
hist(cars_orig$Exposure, main = "", xlab = "")
boxplot(cars_orig$Exposure)
```

### Region {.unnumbered .unlisted}

```{r figRegion, out.width = "50%", out.ncol = 2, fig.cap = "Region", fig.subcap = c("ClaimNb by Region", "Exposure by Region")}
boxplot(data = cars_orig, ClaimNb ~ Region)
boxplot(data = cars_orig, Exposure ~ Region)
boxplot(data = cars_orig, Exposure ~ Region)
```

### Area {.unnumbered .unlisted}

```{r figArea, out.width = "50%", out.ncol = 2, fig.cap = "Area", fig.subcap = c("ClaimNb by Area", "Exposure by Area")}
boxplot(data = cars_orig, ClaimNb ~ Area)
boxplot(data = cars_orig, Exposure ~ Area)
```

### VehBrand {.unnumbered .unlisted}

```{r figVehBrand, out.width = "50%", out.ncol = 2, fig.cap = "VehBrand", fig.subcap = c("ClaimNb by VehBrand", "Exposure by VehBrand")}
boxplot(data = cars_orig, ClaimNb ~ VehBrand)
boxplot(data = cars_orig, Exposure ~ VehBrand)
```

### VehPower {.unnumbered .unlisted}

```{r figVehPower, out.width = "33%", out.ncol = 3, fig.cap = "VehPower", fig.subcap=c("VehPower by Region", "Histogram of VehPower", "Boxplot of VehPower")}
create_map(cars_orig, VehPower, shape, source_or_run = "run")
hist(cars_orig$VehPower, main = "", xlab = "")
boxplot(cars_orig$VehPower)

```

### VehAge {.unnumbered .unlisted}

```{r figVehAge, out.width = "33%", out.ncol = 3, fig.cap = "VehAge", fig.subcap=c("VehAge by Region", "Histogram of VehAge", "Boxplot of VehAge")}
create_map(cars_orig, VehAge, shape, source_or_run = "run")
hist(cars_orig$VehAge, main = "", xlab = "")
boxplot(cars_orig$VehAge)
```

### DrivAge {.unnumbered .unlisted}

```{r figDrivAge, out.width = "33%", out.ncol = 3, fig.cap = "DrivAge", fig.subcap=c("DrivAge by Region", "Histogram of DrivAge", "Boxplot of DrivAge")}
create_map(cars_orig, DrivAge, shape, source_or_run = "run")
hist(cars_orig$DrivAge, main = "", xlab = "")
boxplot(cars_orig$DrivAge, main = "")
```

### BonusMalus {.unnumbered .unlisted}

```{r figBonusMalus, out.width = "33%", out.ncol = 3, fig.cap = "BonusMalus", fig.subcap=c("BonusMalus by Region", "Histogram of BonusMalus", "Boxplot of BonusMalus")}
create_map(cars_orig, BonusMalus, shape, source_or_run = "run")
hist(cars_orig$BonusMalus, main = "")
boxplot(cars_orig$BonusMalus)
```

### Density {.unnumbered .unlisted}

```{r figDensity, out.width = "33%", out.ncol = 3, fig.cap = "Density", fig.subcap=c("Density by Region", "Histogram of Density", "Boxplot of Density")}
create_map(cars_orig, Density, shape, source_or_run = "run")
hist(cars_orig$Density, main = "")
boxplot(cars_orig$Density)
```

\newpage

| Variable | Preprocessing Steps |
|----------|---------------------|
| ClaimNb  | Truncated to 5      |
| Exposure | Truncated to 1      |

: List of Preprocessing Steps

<center>

<center>

```{r figCor, include = FALSE, eval = FALSE, fig.cap = "Correlation Heatmap of Features"}
par(mfrow = c(1, 1))
cars_orig %>%
  mutate(across(where(is.factor), ~ as.numeric(unclass(.)))) %>%
  mutate(VehGas = as.numeric(VehGas == "Diesel")) %>%
  dplyr::select(-IDpol) %>%
  cor() %>%
  heatmap(keep.dendro = FALSE)
```

\newpage

</center>

```{r figBox, echo = FALSE, include = TRUE, fig.path = "data/figures/", fig.cap = "Boxplots of Numerical Variables", fig.subcap = c("ClaimNb", "VehPower", "VehAge", "DrivAge", "Exposure"), out.width="20%", fig.ncol=5}
boxplot(cars_orig$ClaimNb, col = "blue")
boxplot(cars_orig$VehPower, col = "green")
boxplot(cars_orig$VehAge, col = "red")
boxplot(cars_orig$DrivAge, col = "orange")
boxplot(cars_orig$Exposure, col = "purple")
```

```{r figHist, echo = FALSE, include = TRUE, fig.path = "data/figures/", fig.cap = "Histograms of Numerical Variables", fig.subcap = c("ClaimNb", "VehPower", "VehAge", "DrivAge", "Exposure"), out.width="20%", fig.ncol=5, out.height="100%"}
ggplot(cars_orig, aes(x = ClaimNb)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black")

ggplot(cars_orig, aes(x = VehPower)) +
  geom_histogram(binwidth = 1, fill = "green", color = "black")

ggplot(cars_orig, aes(x = VehAge)) +
  geom_histogram(binwidth = 1, fill = "red", color = "black")

ggplot(cars_orig, aes(x = DrivAge)) +
  geom_histogram(binwidth = 1, fill = "orange", color = "black")

ggplot(cars_orig, aes(x = Exposure)) +
  geom_histogram(binwidth = 1 / 12, fill = "purple", color = "black")
```

\newpage

## Modelling

### GLMs

```{r glmSetup, include=TRUE, echo = FALSE}
# Get dataframe and split
cars_split <- cars_orig %>%
  dplyr::select(-IDpol) %>%
  dplyr::select(where(is.factor), everything()) %>%
  initial_split(prop = 0.8)
train <- training(cars_split)
test <- testing(cars_split)
```

Using the basic R implementation [@stats-3] of GLMs, one can fit a model with the following:

```{r glmFit, include  = TRUE, echo=TRUE}
glm1 <- glm(ClaimNb ~ .,
            data = train,
            family = poisson(link = log),
            offset = Exposure)
```

Yielding:

```{r glmSummaryCoef, include = TRUE, echo = FALSE}
summary(glm1)$coefficients %>%
  as_tibble() %>%
  mutate(
    " " = rownames(summary(glm1)$coefficients),
    Significance = case_when(
      `Pr(>|z|)` == 0 ~ "***",
      `Pr(>|z|)` < 0.001 ~ "**",
      `Pr(>|z|)` < 0.01 ~ "*",
      `Pr(>|z|)` < 0.05 ~ ".",
      .default = " "
    )
  ) %>%
  relocate(" ", .before = everything()) %>%
  kable(
    format = "latex", booktabs = TRUE,
    caption = "Coefficients of fitted GLM",
    row.names = TRUE
  ) %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_styling(latex_options = "striped") %>%
  pack_rows(
    index = c(
      " " = 1,
      "Area" = 5,
      "VehBrand" = 10,
      "Region" = 21,
      "Numeric" = 7
    )
  )
```

```{r glmSumDetails, include = TRUE, echo = FALSE}
cat(
  "     Null Deviance:", summary(glm1)$null.deviance, "on",
  summary(glm1)$df.null, "degrees of freedom\n",
  "Residual Deviance:", summary(glm1)$deviance, "on",
  summary(glm1)$df.residual, "degrees of freedom\n",
  "AIC:", summary(glm1)$aic,
  "\n\n",
  "Number of Fisher Scoring iterations", summary(glm1)$iter
)
```

-   Where: “ClaimNb \~ .”: is the formula, signifying ClaimNb as the response variable and including all other variables as predictors

-   “train” is the training set

-   “family” is defined as poisson using log as the link function

Alternatively for more robust feature selection one can use backwards elimination

```{r glmBackElimEcho, echo = TRUE, eval = FALSE}
backward_elimination <- stats::step(glm1, direction = "backward")
```

```{r glmBackElim, include = TRUE, echo = FALSE}
backward_elimination <- get_backward_elimination(formula = glm1)
```

```{r glmBackSummaryCoef, include = TRUE, echo = FALSE}
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
summary(backward_elimination)$coefficients %>%
  as_tibble() %>%
  mutate(
    " " = rownames(summary(backward_elimination)$coefficients),
    Significance = case_when(
      `Pr(>|z|)` == 0 ~ "***",
      `Pr(>|z|)` < 0.001 ~ "**",
      `Pr(>|z|)` < 0.01 ~ "*",
      `Pr(>|z|)` < 0.05 ~ ".",
      .default = " "
    )
  ) %>%
  relocate(" ", .before = everything()) %>%
  kable(
    format = "latex", booktabs = TRUE,
    caption = "Coefficients of GLM with backwards elimination",
    row.names = TRUE
  ) %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_styling(latex_options = "striped") %>%
  pack_rows(
    index = c(
      " " = 1,
      "Area" = 5,
      "VehBrand" = 10,
      "Region" = 21,
      "Numeric" = 6
    )
  )
```

```{r glmBackElimSum, Details, echo = FALSE, include = TRUE}
cat(
  "     Null Deviance:", summary(backward_elimination)$null.deviance, "on",
  summary(backward_elimination)$df.null, "degrees of freedom\n",
  "Residual Deviance:", summary(backward_elimination)$deviance, "on",
  summary(backward_elimination)$df.residual, "degrees of freedom\n",
  "AIC:", summary(backward_elimination)$aic,
  "\n\n",
  "Number of Fisher Scoring iterations", summary(backward_elimination)$iter
)
```

This is the familiar output of a the summary of a GLM, with the intercept, standard error and significance level of each predictor outlined. It can be seen that the categorical predictors have been split into dummy variables in the cases of Region, VehBrand, and Area. Metrics such as Deviance and AIC for the model as a whole can be seen underneath, alternatively we can evaluate performance with other metrics as seen below:

```{r glmMetrics, echo = FALSE, include=TRUE}
predictions <- predict(glm1, newdata = test, type = "response") %>%
  as_tibble() %>%
  mutate(index = row_number())

val_data <- test %>%
  mutate(index = row_number()) %>%
  left_join(predictions, join_by(index))

yardstick::metrics(data = val_data, truth = ClaimNb, estimate = value) %>%
  knitr::kable(
    format = "latex", booktabs = TRUE,
    caption = "Metrics of GLM Predictions on Validation Data"
  ) %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_styling(latex_options = "striped")
```

We can also look at a selection of predictions made from the resulting model:

```{r glmPredictions, include = TRUE, echo = FALSE, out.width = "100%", paged.print = TRUE}
head(val_data) %>%
  dplyr::select(-index) %>%
  relocate(ClaimNb, .before = value) %>%
  mutate(Prediction = value) %>%
  kable(
    format = "latex", booktabs = TRUE,
    caption = "Predictions from GLM on testing data"
  ) %>%
  kable_styling(latex_options = "scale_down") %>%
  kable_styling(latex_options = "striped") %>%
  kable_styling(latex_options = "HOLD_position") %>%
  column_spec(11:12, bold = TRUE)
```

### Neural Networks

```{r nnSetup, include = TRUE, echo = FALSE}
log_info("Preprocess data")
df_n <- cars_orig %>%
  dplyr::select(where(is.numeric)) %>%
  mutate(across(c(where(is.numeric), -IDpol, -ClaimNb), ~ scale_col(.)))

df <- cars_orig %>%
  mutate(ClaimNb = as.integer(ClaimNb)) %>%
  dplyr::select(c(IDpol, !where(is.numeric))) %>%
  mutate(VehGas = as.integer(VehGas == "Diesel")) %>%
  left_join(df_n, join_by(IDpol)) %>%
  dplyr::select(-IDpol)

## Create Recipe ---------------------------------------------------------------
recipe <- recipe(ClaimNb ~ ., data = df) %>%
  step_dummy(all_factor())

## Bake Data -------------------------------------------------------------------
preprocessed_data <- prep(recipe, training = df, retain = TRUE)
baked_data <- bake(preprocessed_data, new_data = NULL)

## Split Data ------------------------------------------------------------------
log_info("Test/train split")
indices <- sample(seq_len(nrow(df)), 0.8 * nrow(df))
train_data <- baked_data[indices, ]
test_data <- baked_data[-indices, ]

x_train <- train_data %>% dplyr::select(-ClaimNb)
x_test <- test_data %>%
  dplyr::select(-ClaimNb) %>%
  as.matrix()

y_train <- train_data$ClaimNb
y_test <- test_data$ClaimNb
```

```{r nnTune, include = TRUE, echo = FALSE}
# Define tuning grid
t_batchsize <- c(512, 1024)
t_epochs <- c(30, 50)
t_act_final <- c("softplus", "exponential")
t_lr <- c(0.01, 0.001)

t_hidden_nodes <- c(16, 32)
t_hidden_act <- c("relu", "tanh")

tune_grid <- expand.grid(
  batchsize = t_batchsize,
  epochs = t_epochs,
  final_act = t_act_final,
  learn_rate = t_lr,
  t_hidden_act1 = t_hidden_act, t_hidden_nodes1 = t_hidden_nodes,
  t_hidden_act2 = t_hidden_act, t_hidden_nodes2 = t_hidden_nodes
)
```

```{r nnFit, echo = FALSE}
# Get list of fits from tuning
if (getOption("source_or_run") == "source") {
  model_list <- readRDS("data/objects/model_list.rds")
}

# Select best fit based on mse
best_fit <- get_best_fit(model_list, tune_grid)

# Fit best model
if (getOption("source_or_run") == "source") {
  best_model <- readRDS("data/objects/best_model.rds")
} else {
  best_model <- fit_keras_poisson(
    x_train,
    y_train,
    nodes = c(best_fit$t_hidden_nodes1, best_fit$t_hidden_nodes2),
    batchsize = best_fit$batchsize,
    n_epochs = best_fit$epochs,
    act_funs = c(
      best_fit$t_hidden_act1,
      best_fit$t_hidden_act2,
      best_fit$final_act
    ),
    lr = best_fit$learn_rate
  )
}
```

```{r nnMetrics, include = TRUE, echo = FALSE}
# Collect Metrics
metrics_df <- data.frame(
  matrix(rep(numeric(256), 5),
    ncol = 5,
    dimnames = list(
      seq_len(256),
      c("index", "loss", "mse", "val_loss", "val_mse")
    )
  )
) %>%
  head() %>%
  kable(
    format = "latex", booktabs = TRUE,
    caption = "Model Tuning Results",
    row.names = FALSE
  ) %>%
  kable_styling(latex_options = "scale_down") %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_styling(latex_options = "striped")
```

Below we can see the architecture of the neural network

```{r nnFigNetwork, results='asis', echo=FALSE}
cat(get_network_tex(
  name = "figNetwork",
  n_input = best_model$params$shape[1],
  n_hidden1 = best_model$params$shape[2],
  n_hidden2 = best_model$params$shape[3]
))
```

Below we can see the computation graph of the neural network

```{r nnCompGraph, results='asis', echo=FALSE}
cat(get_computation_graph_tex(name = "nnCompGraph",
  act_funs = as.character(c(
    best_fit$t_hidden_act1,
    best_fit$t_hidden_act2,
    "exp"
  ))
))
```

## Results

# Discussion

## Introduction

```{=html}
<!--
o Review findings
o Discuss outcomes
o Stake a claim

"The findings of this study clearly show that..."
-->
```
## Evaluation

```{=html}
<!--
o Analyse
o Offer explanations
o Reference the literature
o State implications

"One explanation for..."
-->
```
## Conclusion

```{=html}
<!--
o Limitations
o Recommendations

"This study was limited by..."
-->
```
# Conclusion

\newpage

# References {.unnumbered}

::: {#refs}
:::

# Acknowledgements {.unnumbered}
