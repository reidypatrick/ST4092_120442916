---
title: "Modelling Architectures Underlying Financial Pricing Problems"

date: "`r Sys.Date()`"

author: "Patrick Reidy"

affiliation: "School of Mathematical Sciences, University College Cork"

email: "120442916@umail.ucc.ie"

output: 
  bookdown::pdf_document2:
    fig_crop: no
    keep_tex: FALSE
    toc: FALSE
    extra_dependencies: ["subfig", "tikz"]
  author:
  - name: "Patrick Reidy"
    affiliation: "School of Mathematical Sciences, University College Cork"
    email: "120442916@umail.ucc.ie"
    
header-includes:
  - \usepackage{titling}
  - \pretitle{\begin{center}
    \includegraphics[width=4in,height=2in]{data/figures/University_College_Cork_logo.png}\LARGE\\}
  - \posttitle{\end{center}}
  - \usepackage{tikz}
  - \usetikzlibrary{positioning, arrows.meta, shapes.geometric}
  - \usepackage{amsmath}
  


editor_options: 
  chunk_output_type: inline
  
bibliography: references/fyp_references.bib
csl: https://www.zotero.org/styles/ieee
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(OpenML)
library(tidyverse)
library(tidymodels)
library(raster)
library(ggplot2)
library(gridExtra)
library(knitr)
library(kableExtra)
library(keras)

source("R/functions/source_functions.R", verbose = FALSE)
source_functions()

# config
options(log_verbose = TRUE)
options(source_or_run = "object")

cars_orig <- OpenML::getOMLDataSet(data.id = 41214)$data
```

\newpage

\tableofcontents

\newpage

\listoffigures

\newpage

\listoftables

\newpage

# Glossary {.unnumbered}

# Abstract

## Problem Statement

## Indication of Methodology

## Main Findings

## Principal Conclusion

# Introduction

The field of insurance pricing has witnessed a significant evolution in modelling techniques over the years, with a transition from traditional regression methods to more sophisticated models such as Generalized Linear Models (GLMs) and, most recently, Deep Learning (DL) models.

Linear regression, as the earliest and simplest form of modelling [@su_2012_linear], has been used in a variety of predictive applications due to its ease of interpretation and implementation. However, it struggled to handle the complexities of higher-order applications, such as the representation of non-linear relationships. This is particularly problematic in the context of insurance data, where the outcome is heavily influenced by complex interactions between multiple fields and attributes. Consequently, Generalized Linear Models emerged as a significant advancement, offering a more flexible framework that accommodated various distribution families and data types. GLMs excel in modelling discrete and continuous outcomes while retaining interpretability, making them the go-to choice for many years.

Deep Learning models have recently taken industries across the world by storm, not least the insurance industry, ushering in a new era of predictive accuracy. Neural networks, with their multilayered architecture, can capture intricate patterns in massive datasets, including high-dimensional data, unstructured data, and temporal data. DL models have demonstrated remarkable capabilities in feature extraction, non-linear relationship modelling, and predictive accuracy. They are capable of handling diverse data types such as images, text, and tabular data, which is particularly valuable in the age of big data. These advantages extend to the world of insurance pricing, where their potential remains yet to be completely realised.*TODO: expand!*

The transition from GLMs to Deep Learning is not without its challenges. While DL models excel in predictive performance, they often sacrifice interpretability, which is a crucial requirement in the insurance industry for regulatory compliance and trustworthiness. Additionally, the capabilities of unsupervised learning has been at times overestimated, with many foregoing vital data preprocessing and investigation in the excitement of early demonstrations of these models' performance*TODO: expand!*. Ensuring model fairness *??, expand* and avoiding bias in DL models remains an active area of research, as it is crucial for ethical underwriting and legal compliance. One method to address these concerns is to improve the explainability of these models, or bridge the gap in performance between traditional GLMs and DL models.

## Background

### Linear Regression

Simple regression models are a statistical method to describe the relationship between two variables. They are made up of two components: the Systematic component, and the random component. In the simplest case they take the form:

$$Y = \beta_0 + \beta_1X + \epsilon$$

Where:

-   $Y$ is the dependent or response variable

-   $X$ is the independent or predictor variable

-   $\beta_0$ is the intercept, representing the value of the response when $X = 0$

-   $\beta_1$ is the coefficient that determines the slope and hence the relationship between $X$ and $Y$

-   $\epsilon$ is the error term or noise, representing unexplained variability in the model

The random component here is the $\epsilon$ term, often assumed to be normally distributed, whereas the systematic component is the $\beta_0 + \beta_1X$

This approach can be expanded to incorporate multiple predictor variables and as such takes the form

$$Y = \beta_0 + (\Sigma_{i=1}^p \beta_iX_i) + \epsilon$$

Where:

-   $Y$, $\beta_0$, and $\epsilon$ are the same as before

-   $p$ is the number of predictor variables

-   $X_i, \ i =(1,...,p)$ are the multiple predictor variables

-   $\beta_i, \ i = (1,...,p)$ are the coefficients that represent the relationship between $Y$ and the $X_i$ 's

In order to do this, we must assume the errors are normally distributed

### GLMs

With the advent of Generalised Linear Models discussed by Nelder and Wedderburn[@nelder_1972_generalized], the technique of regression modelling was expanded to yield more robust and effective models. The introduction of a link function is used to capture the non-normal distribution of the data. These are defined as:

$$g(\mu) = \beta_0 + \beta_1X_1 +...+ \beta_pX_p$$

Where:

-   $p$ is the number of predictor variables incorporated into the model

-   $\mu$ is the response variable

-   $g: \mathbb{R} \to \mathbb{R}$ is the link function that relates the expected value of $\mu$ to the linear combination of the predictors

-   $X_1,..., X_p$ are the predictor variables

-   $\beta_1,..., \beta_p$ are the coefficients that determine the slope depending on the values of $X_1,..., X_p$

-   $\beta_0$ represents again the intercept, or the value returned when $X_1,..., X_p$ are all equal to zero

```{=html}
<!--# 
o Establishing a territory
  -> Claiming centrality
  -> Making topic generalisations
  -> Reviewing items of previous research
  
  "In the past decade much research has focused on..."

o Establishing a niche
  -> Counter-claiming
  -> Indicating a gap
  -> Question Raising
  -> Continuing a tradition
  
  "It remains unclear why..."

o Occupying the niche
  -> Outlining purposes
  -> Announcing principal findings
  -> Indicating Structure
  
  "The purpose of this study was to..."
-->
```
### Neural Networks

#### Attempt 1

If we recall the formula for the GLM:

$$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_pX_p + \epsilon$$

We can also write this in matrix notation

$$
Y = \beta_0 + 
\begin{bmatrix} \beta_1, \ldots, \beta_p\end{bmatrix}
\begin{bmatrix} X_1 \\ \vdots \\ X_p\end{bmatrix}
$$

When looking at neural networks, we see a familiar form for a simple network with no hidden layers (these will be discussed further later). A neural network typically takes the form:

$$
Y = \begin{bmatrix} W_{11} & \ldots & W_{1p} \\ 
                    \vdots & \ddots & \vdots \\
                    W_{q1} & \ldots & W_{qp}\end{bmatrix} 
    \begin{bmatrix} X_1 \\ \vdots \\ X_p\end{bmatrix}
$$

There are two key differences between this form and the previous:

-   There is no intercept in the NN formula, meaning the response is not necessarily being made relative to some base case

-   Rather than a row vector of coefficients, we instead use a $n\times p$ matrix of weights. This is especially useful in capturing relationships between variables. Of course the result of this is no longer a single value, but a vector of length $q$ which for now is arbitrarily chosen.

In order to return a single value as desired, we must again multiply the results of this equation by a row vector of length $q$.

$$
Y = \begin{bmatrix}\begin{bmatrix} W_{1,11} & \ldots & W_{2,1p} \\
                    \vdots & \ddots & \vdots \\
                    W_{1,q1} & \ldots & W_{1,qp}\end{bmatrix} 
    \begin{bmatrix} X_1 \\ \vdots \\ X_p\end{bmatrix}\end{bmatrix}
    \begin{bmatrix} W_{2,1} & \ldots & W_{2,q}\end{bmatrix}
$$

Here we now have two matrices of weights, $W_1$ and $W_2$ which results in a scalar output $Y$.

#### Attempt 2

If we recall our GLM formula of

$$log(\mu) = \beta_0 + \beta_1X_1 +...+ \beta_pX_p$$

We can rewrite this in matrix notation as

$$
log(\mu) = \beta_0 + 
\begin{bmatrix} \beta_1, \ldots, \beta_p\end{bmatrix}
\begin{bmatrix} X_1 \\ \vdots \\ X_p\end{bmatrix}
$$

Representing a full model of $p$ independent variables. However, for fitting a saturated model including all possible interactions, very quickly our number of variables grows to an unmanageable size. Computability, memory availability, and interpretability will all suffer. However, we can extend our set of coefficients into a matrix with an arbitrary amount of rows, say $q$ which each represent some relationship over all predictors. This will result in a formula that looks like

$$
           \begin{bmatrix} W_{11} & \ldots & W_{1p} \\ 
                    \vdots & \ddots & \vdots \\
                    W_{q1} & \ldots & W_{qp}\end{bmatrix} 
           \begin{bmatrix} X_1 \\ \vdots \\ X_p\end{bmatrix} 
         = \begin{bmatrix} H_1 & \ldots & H_q\end{bmatrix} 
$$

This results in a vector of length $q$ which represents $q$ relationships from the model. Of course, as we desire a scalar output similar to how a GLM would produce, we would then need to multiply this by a column vector of length $q$ $$
\hat{Y} = \begin{bmatrix}\begin{bmatrix} W_{1,11} & \ldots & W_{2,1p} \\
                    \vdots & \ddots & \vdots \\
                    W_{1,q1} & \ldots & W_{1,qp}\end{bmatrix} 
    \begin{bmatrix} X_1 \\ \vdots \\ X_p\end{bmatrix}\end{bmatrix}
    \begin{bmatrix} W_{2,1} & \ldots & W_{2,q}\end{bmatrix}
$$

Where $\hat{Y}$ is our new scalar output and instead of one matrix of coefficients $W$ we have $W_1$ and $W_2$. It may also at this stage be helpful to make an adjustment similar to what we do for interpreting GLMs where

$$log(\mu) = \beta_0 + \beta_1X_1 +...+ \beta_pX_p$$

And after getting the exponential of both sides, this becomes equivalent to

$$\mu = exp(\beta_0 + \beta_1X_1 +...+ \beta_pX_p)$$

And applying this to our matrix notation we now get

$$
\hat{Y} = exp\left(\begin{bmatrix}\begin{bmatrix} W_{1,11} & \ldots & W_{2,1p} \\ 
                    \vdots & \ddots & \vdots \\
                    W_{1,q1} & \ldots & W_{1,qp}\end{bmatrix} 
    \begin{bmatrix} X_1 \\ \vdots \\ X_p\end{bmatrix}\end{bmatrix}
    \begin{bmatrix} W_{2,1} & \ldots & W_{2,q}\end{bmatrix}\right)
$$ It is clear that with the nesting of these processes, the architecture becomes very difficult to understand. We can write this in a more sequential pattern as following:

$$
  \begin{bmatrix} W_{1,11} & \ldots & W_{2,1p} \\
                  \vdots & \ddots & \vdots \\
                  W_{1,q1} & \ldots & W_{1,qp}\end{bmatrix} 
  \begin{bmatrix} X_1 \\ \vdots \\ X_p\end{bmatrix}
                = \begin{bmatrix} H_{1,1} \\ \vdots \\ H_{1,q} \end{bmatrix}
\Rightarrow
           \begin{bmatrix} H_1 \\ \vdots \\ H_q \end{bmatrix}
    \times \begin{bmatrix} W_{2,1} & \ldots & W_{2,q}\end{bmatrix}
    = H_2
\Rightarrow
    \exp(H_2)
    = \hat{Y}
$$

This can be made even clearer visually by representing vectors and matrices as nodes in a network, and producing a computation graph.

```{=tex}
\begin{figure}
    \centering
      \begin{tikzpicture}[>=stealth, node distance=1.5cm, every node/.style={scale=0.8}]
      % Input layer
      \node[circle, draw, fill=yellow!20] (X) at (0,-1) {$X$};
      \node[circle, draw, fill=blue!20] (W1) at (0,-2) {$W_1$};
      
      \node[circle, draw, fill=red!20] (H1) at (2,-1) {$H_1$};
      \node[circle, draw, fill=green!20] (star1) at (1,-1) {$*$};
      
      % Labels
      \node[above=0.5cm of X] {Input Layer};
      
      % Connect Nodes
      \draw[->] (X) -- (star1);
      \draw[->] (W1) -- (star1);
      \draw[->] (star1) -- (H1);
      
      % First Hidden Layer
      \node[circle, draw, fill=blue!20] (W2) at (2,-2) {$W_2$};
      
      \node[circle, draw, fill=red!20] (H2) at (4,-1) {$H_2$};
      \node[circle, draw, fill=green!20] (star2) at (3,-1) {$*$};
      
      % Connect Nodes
      \draw[->] (W2) -- (star2);
      \draw[->] (H1) -- (star2);
      
      % Final Layer
      \node[circle, draw, fill=green!20] (link) at (5,-1) {$exp$};
      \node[circle, draw, fill=yellow!20] (output) at (6,-1) {$\hat{Y}$};
      
      % Connect Nodes
      \draw[->] (star2) -- (H2);
      \draw[->] (H2) -- (link);
      \draw[->] (link) -- (output);
    
    \end{tikzpicture}
    \caption{Computation Graph of Adapted GLM??}
    \label{fig:simple_comp_graph}
  \end{figure}
```
Where:

-   The yellow nodes $X$, $\hat{Y}$ are the inputs and outputs of our model.

-   The blue nodes $W_1$ and $W_2$ are our matrices of weights.

-   The green nodes are functions, where an incoming arrow indicates inputs and outgoing indicates output.

-   The red nodes are intermediary steps, where the output of a previous function is the input of a subsequent function. *Note: These are displayed here as a visual aid to relate to previous formulae. In practice, these are not necessarily saved nor important, and will cease to be used in future diagrams. They are useful to visualise the concept of "hidden layers", but really this applies more to the functions either side of where these are shown.*

#### Loss Function vs MLE

The parameters $\beta$ in a GLM are found by maximising their likelihood function given the data. Given that we are assuming a poisson distribution, this likelihood function is just the product of the pdfs of the observations.

$$
L(\beta|y) = \prod^n_{i=1} \frac{e^{-\mu_i}\mu_i^{y_i}}{y_i!}
$$

Where $\mu_i = exp(\beta_0 + \beta_1X_{i,1} + \ldots + \beta_pX_{i,p})$. We can now take the log likelihood

$$
L(\beta|y)  = \prod^n_{i=1} \frac{e^{-\mu_i}\mu_i^{y_i}}{y_i!}\\
\ell(\beta|y)   = \sum^n_{i=1} log( \frac{e^{-\mu_i}\mu_i^{y_i}}{y_i!})\\
             = \sum^n_{i=1}-\mu_i + y_ilog(\mu_i) - log(y_i!)
$$

Seeing as $log(y_i!)$ is independent of our parameters we can ignore this when maximising the function, leaving us with:

$$
\ell(\beta|y) = \sum^n_{i=1}-\mu_i + y_ilog(\mu_i)
$$

It is clear that for a large GLM with many predictor variables in $\mu_i$, calculating each step in this function for every observation becomes very expensive. Because of this optimisation functions are employed. These include (the R default) Iteratively Weighted Least Squares (IWLS), Stochastic Gradient Descent (SCG), Newton-Raphson and Fisher Scoring Iteration.

When dealing in Neural Networks, a loss function is used to evaluate the error in the model iteration, which is then minimised by propagating this error backwards through the network, calculating the gradient of the error with respect to each of the weights, and then updating the weights accordingly. This loss function is obtained identically to the Maximum Likelihood Estimate for the GLM where we have

$$
\ell(\lambda|y) = \sum^n_{i=1}-\lambda_i + y_ilog(\lambda_i)
$$

In this case where $\lambda$ is the latest model output. However instead of maximising this function we want to minimise it, which is equivalent to maximising the negative of this function.

$Loss = \lambda + ylog(\lambda)$

*These are calculated in batches of 512/1024 for computational efficiency, here* $\lambda$ *and* $y$ *are to be* *understood as vectors with length of batchsize. Could just be written as sum as well.*

Once again, computational constraints require that some method is used to optimise this process, for NNs these include SCG again, Adam, etc.

It can be shown that there is no mathematical difference in what a simple NN of 0 hidden layers with exponential activation function produces and that of a GLM, though they're may be computational differences.

# Outline

## Linear Regression

Linear regression dates back to the early 19th century as a method for modellin the relationship between two variables, $Y$ and $X$. They operate on three key assumptions:

1.  The observations or the $y_is$ are independent

2.  The observations are normally distributed with $y_i \sim N(\mu_i, \sigma^2)$

3.  $\mu_i = X_i^T\beta = \beta_0 + \sum_{i=1}^n \beta_iX_i$

With this we can create a model of $Y$ of the form:

$$
Y = \beta_0 + \sum^n_{i=1}\beta_iXi
$$

These $\beta$ parameters are determined using a Least Squares Estimate (LSE)

$$
\min \sum^n_{i=1}(y_i-\mu_i)^2 = \min\sum^n_{i=1}(y_i-(\beta_0 + \sum^n_{i=1}\beta_iXi))^2
$$

## Generalised Linear Models

Generalised Linear Models (GLMs) were an extension or in fact as the name suggests a generalisation of this form to all distributions in the Exponential Dispersion Family (EDF) which expanded the assumption such that:

1.  The observations or the $y_is$ are independent

2.  $y_i \sim$ Exponential Dispersion Family, which take the form:

    $$
    f(y) = \exp(\frac{1}{a(\phi}(y\theta-b(\theta)) + c(y,\phi)
    $$

3.  There is some $g(\mu_i) = X_i^T\beta = \beta_0 + \sum_{i=1}^n \beta_iX_i$

What we find is that for distributions in this class, this function $g$, known as the link function can be derived from the form in assumption (2). For the poisson family, we find that this function is the $log$ function. Meaning we can construct a model of the form:

$$
log(\mu_i) = \beta_0 + \sum^n_{i=1}\beta_iXi
$$

We can take this to mean that while we can not fit a linear model to out target variable, we can fit a linear model to the log of our target variable.

Another key difference was the use of a Maximum Likelihood Estimator (MLE) in place of an LSE. Of course for a Normal Distribution, these are the same. However, for other distributions in the EDF, this is not the case. We derive the MLE for a poisson distribution starting with the Likelihood function of the $\beta s$ given the true data as:

$$ L(\beta|y) = \prod^n_{i=1} \frac{e^{-\mu_i}\mu_i^{y_i}}{y_i!} $$

Where $\mu_i = exp(\beta_0 + \beta_1X_{i,1} + \ldots + \beta_pX_{i,p})$. We can now take the log likelihood

$$ L(\beta|y)  = \prod^n_{i=1} \frac{e^{-\mu_i}\mu_i^{y_i}}{y_i!}\\ \ell(\beta|y)   = \sum^n_{i=1} log( \frac{e^{-\mu_i}\mu_i^{y_i}}{y_i!})\\              = \sum^n_{i=1}-\mu_i + y_ilog(\mu_i) - log(y_i!) $$

Seeing as $log(y_i!)$ is independent of our parameters we can ignore this when maximising the function, leaving us with:

$$ \ell(\beta|y) = \sum^n_{i=1}-\mu_i + y_ilog(\mu_i) $$

It is clear that for a large GLM with many predictor variables in $\mu_i$, calculating each step in the maximisation function for every observation becomes very expensive. Because of this optimisation functions are employed. These include (the R default) Iteratively Weighted Least Squares (IWLS), Stochastic Gradient Descent (SCG), Newton-Raphson and Fisher Scoring Iteration. These are all different methods of finding the optimal $\beta$ in order to maximise likelihood.

## Neural Networks

Neural Networks (NNs), while possibly theorised in the mid 20th century, only began realising their potential in recent decades, as computational resources began to enable the architecture to outperform classical statistical models. A neural network consists of multiple layers, each containing nodes. Typically, a NN will have an input layer, which takes in a vector of inputs, multiplies this with a matrix of weights, adds a bias term to each output, and then applies an activation function which is used to capture non-linearity in the data, much like we have seen with the link function in GLMs, namely the log function in the context of Poisson GLMs. This yields

$$ \hat{y} = 
\begin{bmatrix} W_{11} & \ldots & W_{1p} \\ 
                    \vdots & \ddots & \vdots \\
                    W_{q1} & \ldots & W_{qp}\end{bmatrix} 
  \begin{bmatrix} X_1 \\ \vdots \\ X_p\end{bmatrix} 
+ \begin{bmatrix}b_1 \\ \vdots \\ b_p\end{bmatrix}
$$

$$
Y = [WX+b \Rightarrow Activation Function]
$$

Where:

-   Y is the target or response variable

-   X is the vector of predictor variables, say of length $p$

-   W is the matrix of weights, with $p$ columns and say some arbitrary $q$ rows.

-   $ActivationFunction$ is some function that augments the result to capture non-linearity.

This form only applies to the input layer of the model, as the resultant value(s) would then be fed into a "hidden" layer, where this procedure $WX+b \Rightarrow ActivationFunction$ is again applied, except X is now the output of the previous layer. As may already be clear, it can become quite cumbersome to notate this process, and so NNs are often visualised with computation graphs, one of which for a NN with a typical structure can be seen below:

```{r, figDummyCompGraph, results="asis", echo=FALSE}
cat(get_computation_graph_tex(
  act_funs = c("", "", ""),
  name = "figDummyCompGraph"
))
```

Where:

-   The yellow nodes $X$, $\hat{y}$ are the inputs and outputs of our model respectively, while $y$ is the true value of the target variable.

-   The blue nodes $W_1$ and $W_2$ are our matrices of weights.

-   The green nodes are functions, where an incoming arrow indicates inputs and outgoing indicates output.

-   The red nodes are the activation functions, which capture the non-linearity in the model.

When dealing in Neural Networks, a loss function is used to evaluate the error in the model iteration, which is then minimised by propagating this error backwards through the network, calculating the gradient of the error with respect to each of the weights, and then updating the weights accordingly. This loss function is obtained identically to the Maximum Likelihood Estimate for the GLM where we have

$$ \ell(\lambda|y) = \sum^n_{i=1}-\lambda_i + y_ilog(\lambda_i) $$

In this case where $\lambda$ is the latest model output. However instead of maximising this function we want to minimise it, which is equivalent to maximising the negative of this function.

$Loss = \lambda + ylog(\lambda)$

*These are calculated in batches of 512/1024 for computational efficiency, here* $\lambda$ *and* $y$ *are to be* *understood as vectors with length of batchsize. Could just be written as sum as well.*

Once again, computational constraints require that some method is used to optimise this process, for NNs these include SCG again, Adam, etc.

## Bringing it All Together

If we recall our GLM formula of

$$log(\mu) = \beta_0 + \beta_1X_1 +...+ \beta_pX_p$$

We can rewrite this in matrix notation as

$$ log(\mu) = \beta_0 +  \begin{bmatrix} \beta_1, \ldots, \beta_p\end{bmatrix} \begin{bmatrix} X_1 \\ \vdots \\ X_p\end{bmatrix} $$

We can rearrange this as:

$$ log(\mu) = \begin{bmatrix} \beta_1, \ldots, \beta_p\end{bmatrix} \begin{bmatrix} X_1 \\ \vdots \\ X_p\end{bmatrix} + \beta_0 $$

# Literature Review

# Methodology

```{=html}
<!--
"The data used for this study were collected by..."
-->
```
## Data Collection

This study is carried out using the French motor third partly liability claims frequency data (FreMTPL2freq) available through CASdatasets [@CASdatasets].

```{r tabHead, echo=FALSE, include=TRUE, paged.print=TRUE}
head(cars_orig) %>%
  kable(
    format = "latex", booktabs = TRUE,
    caption = "Structure of the FreMTPL2freq Dataset",
    row.names = FALSE
  ) %>%
  kable_styling(latex_options = "scale_down") %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_styling(latex_options = "striped")
```

The description of the data as outlined in the documentation is as follows:

*"Context: In the dataset freMTPL2freq risk features and claim numbers were collected for 677,991 motor third-part liability policies (observed on a year). Content freMTPL2freq contains 11 columns*

-   *IDpol: The policy ID (used to link with the claims dataset).*

-   *ClaimNb: Number of claims during the exposure period.* This is the variable to be predicted

-   *Exposure: The exposure period.*

-   *Area: The area code.* Six distinct area codes from A to F.

-   *VehPower: The power of the car (ordered categorical).*

-   *VehAge: The vehicle age, in years.*

-   *DrivAge: The driver age, in years (in France, people can drive a car at 18).*

-   *BonusMalus: Bonus/malus, between 50 and 350: 100 means malus in France.*

-   *VehBrand: The car brand (unknown categories).*

-   *VehGas: The car gas, Diesel or regular.*

-   *Density: The density of inhabitants (number of inhabitants per km2) in the city in which the driver of the car lives.*

-   *Region: The policy regions in France (based on a standard French classification) "*

```{r tableSum, echo=FALSE, include=TRUE, paged.print=TRUE}
cars_orig %>%
  mutate(VehGas = as.factor(VehGas)) %>%
  dplyr::select(-IDpol, -where(is.factor)) %>%
  summary() %>%
  knitr::kable(
    format = "latex", booktabs = TRUE,
    caption = "Summary of Variables in Data"
  ) %>%
  kable_styling(latex_options = "scale_down") %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_styling(latex_options = "striped")
```

## Data Analysis

```{r setup_fig, echo=FALSE}
wd <- file.path("C:/Users/patos/Documents/ST4092_120442916")
dsn <- file.path(wd, "data/input/shapefile")
```

Before investigating the performance of the models, we carry out a number of investigations regarding the structure and behaviour of the data. This is important in identifying which methods to use, issues to address and results to expect.

*Should first step be investigating e.g. cor matrix, i.e. dataset as a whole?*

As ClaimNb is the response variable it is no doubt the most important variable to understand and the first variable we investigate. Figure \@ref(fig:fig_cor)

*Discuss each plot in turn and insights yielded, Motivation behind each plot*

*Histogram of every factor vs ClaimNb - Area, Vehpower, VehAge, VehBrand, VehGas, Region* *Scatterplot of every numeric vs ClaimNb - Exposure, DrivAge, Density* *Map of every variable*

See Figure \@ref(fig:figHist-1)

See Figure \@ref(fig:figHist-2)

See Figure \@ref(fig:figHist-3)

See Figure \@ref(fig:figHist-4)

\newpage

<center>

<center>

```{r figCor, echo=FALSE, fig.cap = "Correlation Heatmap of Features", fig.path = "data/figures/"}
par(mfrow = c(1, 1))
cars_orig %>%
  mutate(across(where(is.factor), ~ as.numeric(unclass(.)))) %>%
  mutate(VehGas = as.numeric(VehGas == "Diesel")) %>%
  dplyr::select(-IDpol) %>%
  cor() %>%
  heatmap(keep.dendro = FALSE)
```

\newpage

</center>

```{r figBox, echo = FALSE, include = TRUE, fig.path = "data/figures/", fig.cap = "Boxplots of Numerical Variables", fig.subcap = c("ClaimNb", "VehPower", "VehAge", "DrivAge", "Exposure"), out.width="20%", fig.ncol=5}
boxplot(cars_orig$ClaimNb, col = "blue")
boxplot(cars_orig$VehPower, col = "green")
boxplot(cars_orig$VehAge, col = "red")
boxplot(cars_orig$DrivAge, col = "orange")
boxplot(cars_orig$Exposure, col = "purple")
```

```{r figHist, echo = FALSE, include = TRUE, fig.path = "data/figures/", fig.cap = "Histograms of Numerical Variables", fig.subcap = c("ClaimNb", "VehPower", "VehAge", "DrivAge", "Exposure"), out.width="20%", fig.ncol=5, out.height="100%"}
ggplot(cars_orig, aes(x = ClaimNb)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black")

ggplot(cars_orig, aes(x = VehPower)) +
  geom_histogram(binwidth = 1, fill = "green", color = "black")

ggplot(cars_orig, aes(x = VehAge)) +
  geom_histogram(binwidth = 1, fill = "red", color = "black")

ggplot(cars_orig, aes(x = DrivAge)) +
  geom_histogram(binwidth = 1, fill = "orange", color = "black")

ggplot(cars_orig, aes(x = Exposure)) +
  geom_histogram(binwidth = 1 / 12, fill = "purple", color = "black")
```

\newpage

## Modelling

### GLMs

```{r glmSetup, include=TRUE, echo = FALSE}
# Get dataframe and split
cars_split <- cars_orig %>%
  dplyr::select(-IDpol) %>%
  dplyr::select(where(is.factor), everything()) %>%
  initial_split(prop = 0.8)
train <- training(cars_split)
test <- testing(cars_split)
```

Using the basic stats \@[@stats-3] implementation of GLMs, one can fit a model with the following:

```{r glmFit, include  = TRUE, echo=TRUE}
glm1 <- glm(ClaimNb ~ .,
            data = train,
            family = poisson(link = log),
            offset = Exposure)
```

Yielding:

```{r glmSummaryCoef, include = TRUE, echo = FALSE}
summary(glm1)$coefficients %>%
  as_tibble() %>%
  mutate(
    " " = rownames(summary(glm1)$coefficients),
    Significance = case_when(
      `Pr(>|z|)` == 0 ~ "***",
      `Pr(>|z|)` < 0.001 ~ "**",
      `Pr(>|z|)` < 0.01 ~ "*",
      `Pr(>|z|)` < 0.05 ~ ".",
      .default = " "
    )
  ) %>%
  relocate(" ", .before = everything()) %>%
  kable(
    format = "latex", booktabs = TRUE,
    caption = "Coefficients of fitted GLM",
    row.names = TRUE
  ) %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_styling(latex_options = "striped") %>%
  pack_rows(
    index = c(
      " " = 1,
      "Area" = 5,
      "VehBrand" = 10,
      "Region" = 21,
      "Numeric" = 7
    )
  )
```

```{r glmSumDetails, include = TRUE, echo = FALSE}
cat(
  "     Null Deviance:", summary(glm1)$null.deviance, "on",
  summary(glm1)$df.null, "degrees of freedom\n",
  "Residual Deviance:", summary(glm1)$deviance, "on",
  summary(glm1)$df.residual, "degrees of freedom\n",
  "AIC:", summary(glm1)$aic,
  "\n\n",
  "Number of Fisher Scoring iterations", summary(glm1)$iter
)
```

-   Where: “ClaimNb \~ .”: is the formula, signifying ClaimNb as the response variable and including all other variables as predictors

-   “train” is the training set

-   “family” is defined as poisson using log as the link function

Alternatively for more robust feature selection one can use backwards elimination

```{r glmBackElimEcho, include = FALSE, echo = TRUE}

```

```{r glmBackElim, include = TRUE, echo = FALSE}
if (global_variables$source == TRUE) {
  backward_elimination <- get_backward_elimination("object")
} else {
  backward_elimination <- get_backward_elimination("run")
}
```

```{r glmBackSummaryCoef, include = TRUE, echo = FALSE}
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
summary(backward_elimination)$coefficients %>%
  as_tibble() %>%
  mutate(
    " " = rownames(summary(backward_elimination)$coefficients),
    Significance = case_when(
      `Pr(>|z|)` == 0 ~ "***",
      `Pr(>|z|)` < 0.001 ~ "**",
      `Pr(>|z|)` < 0.01 ~ "*",
      `Pr(>|z|)` < 0.05 ~ ".",
      .default = " "
    )
  ) %>%
  relocate(" ", .before = everything()) %>%
  kable(
    format = "latex", booktabs = TRUE,
    caption = "Coefficients of GLM with backwards elimination",
    row.names = TRUE
  ) %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_styling(latex_options = "striped") %>%
  pack_rows(
    index = c(
      " " = 1,
      "Area" = 5,
      "VehBrand" = 10,
      "Region" = 21,
      "Numeric" = 6
    )
  )
```

```{r summary(backward_elimination)Details, echo = FALSE, include = TRUE}
cat(
  "     Null Deviance:", summary(backward_elimination)$null.deviance, "on",
  summary(backward_elimination)$df.null, "degrees of freedom\n",
  "Residual Deviance:", summary(backward_elimination)$deviance, "on",
  summary(backward_elimination)$df.residual, "degrees of freedom\n",
  "AIC:", summary(backward_elimination)$aic,
  "\n\n",
  "Number of Fisher Scoring iterations", summary(backward_elimination)$iter
)
```

This is the familiar output of a the summary of a GLM, with the intercept, standard error and significance level of each predictor outlined. It can be seen that the categorical predictors have been split into dummy variables in the cases of Region, VehBrand, and Area. Metrics such as Deviance and AIC for the model as a whole can be seen underneath, alternatively we can evaluate performance with other metrics as seen below:

```{r glmMetrics, echo = FALSE, include=TRUE}
predictions <- predict(glm1, newdata = test, type = "response") %>%
  as_tibble() %>%
  mutate(index = row_number())

val_data <- test %>%
  mutate(index = row_number()) %>%
  left_join(predictions, join_by(index))

yardstick::metrics(data = val_data, truth = ClaimNb, estimate = value) %>%
  knitr::kable(
    format = "latex", booktabs = TRUE,
    caption = "Metrics of GLM Predictions on Validation Data"
  ) %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_styling(latex_options = "striped")
```

We can also look at a selection of predictions made from the resulting model:

```{r glmPredictions, include = TRUE, echo = FALSE, out.width = "100%", paged.print = TRUE}
head(val_data) %>%
  dplyr::select(-index) %>%
  relocate(ClaimNb, .before = value) %>%
  mutate(Prediction = value) %>%
  kable(
    format = "latex", booktabs = TRUE,
    caption = "Predictions from GLM on testing data"
  ) %>%
  kable_styling(latex_options = "scale_down") %>%
  kable_styling(latex_options = "striped") %>%
  kable_styling(latex_options = "HOLD_position") %>%
  column_spec(11:12, bold = TRUE)
```

### Neural Networks

```{r nnSetup, include = TRUE, echo = FALSE}
log_info("Preprocess data")
df_n <- cars_orig %>%
  dplyr::select(where(is.numeric)) %>%
  mutate(across(c(where(is.numeric), -IDpol, -ClaimNb), ~ scale_col(.)))

df <- cars_orig %>%
  mutate(ClaimNb = as.integer(ClaimNb)) %>%
  dplyr::select(c(IDpol, !where(is.numeric))) %>%
  mutate(VehGas = as.integer(VehGas == "Diesel")) %>%
  left_join(df_n, join_by(IDpol)) %>%
  dplyr::select(-IDpol)

## Create Recipe ---------------------------------------------------------------
recipe <- recipe(ClaimNb ~ ., data = df) %>%
  step_dummy(all_factor())

## Bake Data -------------------------------------------------------------------
preprocessed_data <- prep(recipe, training = df, retain = TRUE)
baked_data <- bake(preprocessed_data, new_data = NULL)

## Split Data ------------------------------------------------------------------
log_info("Test/train split")
indices <- sample(seq_len(nrow(df)), 0.8 * nrow(df))
train_data <- baked_data[indices, ]
test_data <- baked_data[-indices, ]

x_train <- train_data %>% dplyr::select(-ClaimNb)
x_test <- test_data %>%
  dplyr::select(-ClaimNb) %>%
  as.matrix()

y_train <- train_data$ClaimNb
y_test <- test_data$ClaimNb
```

```{r nnTune, include = TRUE, echo = FALSE}
# Define tuning grid
t_batchsize <- c(512, 1024)
t_epochs <- c(30, 50)
t_act_final <- c("softplus", "exponential")
t_lr <- c(0.01, 0.001)

t_hidden_nodes <- c(16, 32)
t_hidden_act <- c("relu", "tanh")

tune_grid <- expand.grid(
  batchsize = t_batchsize,
  epochs = t_epochs,
  final_act = t_act_final,
  learn_rate = t_lr,
  t_hidden_act1 = t_hidden_act, t_hidden_nodes1 = t_hidden_nodes,
  t_hidden_act2 = t_hidden_act, t_hidden_nodes2 = t_hidden_nodes
)
```

```{r nnFit, echo = FALSE}
# Get list of fits from tuning
if (global_variables$source == TRUE) {
  model_list <- readRDS("data/objects/model_list.rds")
}

# Select best fit based on mse
best_fit <- get_best_fit(model_list, tune_grid)

# Fit best model
if (global_variables$source == TRUE) {
  best_model <- readRDS("data/objects/best_model.rds")
} else {
  best_model <- fit_keras_poisson(
    x_train,
    y_train,
    nodes = c(best_fit$t_hidden_nodes1, best_fit$t_hidden_nodes2),
    batchsize = best_fit$batchsize,
    n_epochs = best_fit$epochs,
    act_funs = c(
      best_fit$t_hidden_act1,
      best_fit$t_hidden_act2,
      best_fit$final_act
    ),
    lr = best_fit$learn_rate
  )
}
```

```{r nnMetrics, include = TRUE, echo = FALSE}
# Collect Metrics
metrics_df <- data.frame(
  matrix(rep(numeric(256), 5),
    ncol = 5,
    dimnames = list(
      seq_len(256),
      c("index", "loss", "mse", "val_loss", "val_mse")
    )
  )
) %>%
  head() %>%
  kable(
    format = "latex", booktabs = TRUE,
    caption = "Model Tuning Results",
    row.names = FALSE
  ) %>%
  kable_styling(latex_options = "scale_down") %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_styling(latex_options = "striped")
```

Below we can see the architecture of the neural network

```{r, nnFigNetwork, results='asis', echo=FALSE}
cat(get_network_tex(
  n_input = best_model$params$shape[1],
  n_hidden1 = best_model$params$shape[2],
  n_hidden2 = best_model$params$shape[3]
))
```

Below we can see the computation graph of the neural network

```{r, nnCompGraph, results='asis', echo=FALSE}
cat(get_computation_graph_tex(name = "nnCompGraph",
  act_funs = as.character(c(
    best_fit$t_hidden_act1,
    best_fit$t_hidden_act2,
    "exp"
  ))
))
```

## Results

# Discussion

## Introduction

```{=html}
<!--
o Review findings
o Discuss outcomes
o Stake a claim

"The findings of this study clearly show that..."
-->
```
## Evaluation

```{=html}
<!--
o Analyse
o Offer explanations
o Reference the literature
o State implications

"One explanation for..."
-->
```
## Conclusion

```{=html}
<!--
o Limitations
o Recommendations

"This study was limited by..."
-->
```
# Conclusion

# Acknowledgements

# References

::: {#refs}
:::
