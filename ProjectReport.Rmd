---
title: "Modelling Architectures Underlying Financial Pricing Problems"

date: "`r Sys.Date()`"

author: "Patrick Reidy"

output: 
  bookdown::pdf_document2:
    fig_crop: no
    keep_tex: FALSE
  author:
  - name: "Patrick Reidy"
    affiliation: "School of Mathematical Sciences, University College Cork"
    email: "120442916@umail.ucc.ie"

editor_options: 
  chunk_output_type: inline
  
bibliography: references/fyp_references.bib
csl: https://www.zotero.org/styles/ieee
nocite: '@*'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(CASdatasets)
library(OpenML)
library(farff)
library(tidyverse)
library(tidymodels)
library(stringi)
library(sf)
library(raster)
library(ggplot2)
library(broom)
library(gridExtra)


source("code/functions/create_map.R")

data <- OpenML::getOMLDataSet(data.id = 41214)

cars_orig <- data$data
```

# Abstract

## Problem Statement

## Indication of Methodology

## Main Findings

## Principal Conclusion

# Introduction

The field of insurance pricing has witnessed a significant evolution in modelling techniques over the years, with a transition from traditional regression methods to more sophisticated models such as Generalized Linear Models (GLMs) and, most recently, Deep Learning (DL) models.

Linear regression, as the earliest and simplest form of modelling [1], has been used in a variety of predictive applications due to its ease of interpretation and implementation. However, it struggled to handle the complexities of higher-order applications, such as the representation of non-linear relationships. This is particularly problematic in the context of insurance data, where the outcome is heavily influenced by complex interactions between multiple fields and attributes. Consequently, Generalized Linear Models emerged as a significant advancement, offering a more flexible framework that accommodated various distribution families and data types. GLMs excel in modelling discrete and continuous outcomes while retaining interpretability, making them the go-to choice for many years. 

Deep Learning models have recently taken industries across the world by storm, not least the insurance industry, ushering in a new era of predictive accuracy. Neural networks, with their multilayered architecture, can capture intricate patterns in massive datasets, including high-dimensional data, unstructured data, and temporal data. DL models have demonstrated remarkable capabilities in feature extraction, non-linear relationship modelling, and predictive accuracy. They are capable of handling diverse data types such as images, text, and tabular data, which is particularly valuable in the age of big data. These advantages extend to the world of insurance pricing, where their potential remains yet to be completely realised.*TODO: expand!*

The transition from GLMs to Deep Learning is not without its challenges. While DL models excel in predictive performance, they often sacrifice interpretability, which is a crucial requirement in the insurance industry for regulatory compliance and trustworthiness. Additionally, the capabilities of unsupervised learning has been at times overestimated, with many foregoing vital data preprocessing and investigation in the excitement of early demonstrations of these models' performance*TODO: expand!*. Ensuring model fairness *??, expand* and avoiding bias in DL models remains an active area of research, as it is crucial for ethical underwriting and legal compliance. One method to address these concerns is to improve the explainability of these models, or bridge the gap in performance between traditional GLMs and DL models.

## Background

### Linear Regression

Simple regression models are a statistical method to describe the relationship between two variables. They are made up of two components: the Systematic component, and the random component. In the simplest case they take the form:

$$Y = \beta_0 + \beta_1X + \epsilon$$

Where:

-   $Y$ is the dependent or response variable

-   $X$ is the independent or predictor variable

-   $\beta_0$ is the intercept, representing the value of the response when $X = 0$

-   $\beta_1$ is the coefficient that determines the slope and hence the relationship between $X$ and $Y$

-   $\epsilon$ is the error term or noise, representing unexplained variability in the model

The random component here is the $\epsilon$ term, often assumed to be normally distributed, whereas the systematic component is the $\beta_0 + \beta_1X$

This approach can be expanded to incorporate multiple predictor variables and as such takes the form

$$Y = \beta_0 + (\Sigma_{i=1}^p \beta_iX_i) + \epsilon$$

Where:

-   $Y$, $\beta_0$, and $\epsilon$ are the same as before

-   $p$ is the number of predictor variables

-   $X_i, \ i =(1,...,p)$ are the multiple predictor variables

-   $\beta_i, \ i = (1,...,p)$ are the coefficients that represent the relationship between $Y$ and the $X_i$ 's


*TODO: Expand on each of the above, Underlying assumptions, link between linear regression and GLM*

### GLMs

With the advent of Generalised Linear Models discussed by Nelder and Wedderburn[2], the technique of regression modelling was expanded to yield more robust and effective models. The introduction of a link function is used to capture the non-normal distribution of the data. These are defined as:

$$g(\mu) = \beta_0 + \beta_1X_1 +...+ \beta_pX_p$$

Where:

-   $p$ is the number of predictor variables incorporated into the model

-   $\mu$ is the response variable

-   $g: \mathbb{R} \to \mathbb{R}$ is the link function that relates the expected value of $\mu$ to the linear combination of the predictors

-   $X_1,..., X_p$ are the predictor variables

-   $\beta_1,..., \beta_p$ are the coefficients that determine the slope depending on the values of $X_1,..., X_p$

-   $\beta_0$ represents again the intercept, or the value returned when $X_1,..., X_p$ are all equal to zero

```{=html}
<!--# 
o Establishing a territory
  -> Claiming centrality
  -> Making topic generalisations
  -> Reviewing items of previous research
  
  "In the past decade much research has focused on..."

o Establishing a niche
  -> Counter-claiming
  -> Indicating a gap
  -> Question Raising
  -> Continuing a tradition
  
  "It remains unclear why..."

o Occupying the niche
  -> Outlining purposes
  -> Announcing principal findings
  -> Indicating Structure
  
  "The purpose of this study was to..."
-->
```
# Literature Review

# Methodology

```{=html}
<!--
"The data used for this study were collected by..."
-->
```
## Data Collection

*TODO: Answer:*

*What data set is being used, Discuss each plot in turn and insights yielded, Motivation behind each plot*

## Data Analysis

\newpage

<center>

```{r Figure1, fig.cap="Number of Claims by Region", out.width = "100%", fig.align = "center", fig.path = "data/figures/"}
wd <- file.path("C:/Users/patos/Documents/ST4092_120442916")
dsn <- file.path(wd, "data/input/shapefile")

shape <- shapefile(file.path(dsn, "FRA_adm1.shp"))

claim_map <- create_map(cars_orig, ClaimNb, shape)
density_map <- create_map(cars_orig, Density, shape)

grid.arrange(claim_map, density_map, ncol = 2)
```

</center>

\newpage

<center>

```{r Figure2, echo=FALSE, fig.cap = "Correlation Heatmap of Features", fig.path = "data/figures/"}
par(mfrow = c(1, 1))
cars_numeric <- cars_orig %>%
  mutate(across(where(is.factor), ~ as.numeric(unclass(.)))) %>%
  mutate(VehGas = as.numeric(VehGas == "Diesel")) %>%
  dplyr::select(-IDpol)

cor(cars_numeric) %>%
  heatmap(keep.dendro = FALSE)
```

</center>

\newpage

## Modelling

## Results

# Discussion

## Introduction

```{=html}
<!--
o Review findings
o Discuss outcomes
o Stake a claim

"The findings of this study clearly show that..."
-->
```
## Evaluation

```{=html}
<!--
o Analyse
o Offer explanations
o Reference the literature
o State implications

"One explanation for..."
-->
```
## Conclusion

```{=html}
<!--
o Limitations
o Recommendations

"This study was limited by..."
-->
```
# Conclusion

# Acknowledgements

# References

::: {#refs}
:::
