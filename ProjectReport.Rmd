---
title: "Modelling Architectures Underlying Financial Pricing Problems"

date: "`r Sys.Date()`"

author: "Patrick Reidy"

affiliation: "School of Mathematical Sciences, University College Cork"

email: "120442916@umail.ucc.ie"

fontsize: 12pt

output: 
  bookdown::pdf_document2:
    fig_crop: no
    keep_tex: TRUE
    toc: FALSE
    extra_dependencies: ["subfig", "tikz", "float"]
  author:
  - name: "Patrick Reidy"
    affiliation: "School of Mathematical Sciences, University College Cork"
    email: "120442916@umail.ucc.ie"
    
header-includes:
  - \usepackage{titling}
  - \pretitle{\begin{center}
    \includegraphics[width=4in,height=2in]{data/figures/University_College_Cork_logo.png}\LARGE\\}
  - \posttitle{\end{center}}
  - \usepackage{tikz}
  - \usetikzlibrary{positioning, arrows.meta, shapes.geometric}
  - \usepackage{amsmath}
  


editor_options: 
  chunk_output_type: inline
  
bibliography: references/fyp_references.bib
csl: https://www.zotero.org/styles/ieee
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(paged.print = TRUE)
knitr::opts_chunk$set(fig.path = "data/figures/")
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")

library(OpenML)
library(tidyverse)
library(tidymodels)
library(raster)
library(ggplot2)
library(gridExtra)
library(knitr)
library(kableExtra)
library(keras)

source("R/functions/source_functions.R", verbose = FALSE)
source_functions()

# config
options(log_verbose = FALSE)
options(source_or_run = "source")

cars_orig <- OpenML::getOMLDataSet(data.id = 41214)$data
```

\newpage

# Abstract {.unlisted .unnumbered}

\clearpage

\newpage

\tableofcontents

\newpage

\listoffigures

\newpage

\listoftables

\newpage

# Glossary {.unnumbered}

GLM

:   Generalised Linear Model

NN

:   Neural Network

FFNN

:   Feed Forward Neural Network

LSE

:   Least Squares Estimator

MLE

:   Maximum Likelihood Estimator

EDF

:   Exponential Distribution Family

\newpage

# Introduction

Insurance is a transaction where risk is transferred from the policyholder to the insurer, in return for the premium paid by the policyholder. Insurers must calculate this premium and hence this becomes a problem in risk assessment. A policyholder purchases a policy to cover them for a period of time, so that in the event of unpredictable losses and damages, they are covered financially. If not insured, damages would be significant to be reimburse for the individual. In order to determine the necessary premium charged by the insurer, the risk posed by the policyholder must be evaluated.

Insurers aim to balance the loss ratio i.e. the maount of loss from claims to be paid out versus the total premium income from all policyholders.

The insurance premium is usually derived from an annual frequency of claims , which is modelled using statistical data. The annual frequency of claims is calculated from the number of claims on a contract. Thus consideration of factors that influence loss frequency, the risk-taking and risk-reducing behaviour of individuals and companies is crucial. Explanatory (rating) variables can have different effects on models of how often an event occurs. Explanatory variables include age, vehicle type. Prior claims history is also important as a risk classification factor. Not all policyholders have the same probability to claim, therefore it is important that both premiums are determined carefully and policyholders chosen wisely.

In this study we concentrate on statistical models used to determine this risk in particular by means of regression modelling. An aspect of the risk we will consider is the number of claims made by a policyholder over the period of cover.

The approach to insurance pricing has witnessed a significant evolution in modelling techniques over the years, as the capacity of statistical models have advanced from classical regression methods to more sophisticated models such as Generalised Linear Models (GLMs) and, most recently, Deep Learning (DL) models. Insurance providers are constantly searching for ways to improve and gauge the performance of these models. We will discuss several of these models, see how they are related and discuss ongoing developments in the insurance pricing industry.

Third Party Liability (TPL) insurance is that which protects the first party (the policyholder) from legal liability to a third party. It is the minimum level of cover offered in car insurance and required by law in most countries. This will cover the policyholder in the event that they are at fault for any damages and losses owed to the third party, which here is the other driver in the accident. In this paper we will be using a particular dataset of French Motor TPL claims, which is further discussed in Section \@ref(data-collection).

Several Assumptions are adopted in actuarial rating methods, for example in TPL insurance

-   All individual claims are independent meaning

    -   Independence of policies

    -   Time Independence *Expand*

-   Homogeneity

    -   While the data as a *whole* might not be homogeneous, we assume the response from policies in the *same risk category* are, and given they have the same exposure, we then assume that they are from the same probability distribution

This assumption of homogeneity is perhaps the most violated, as although policyholders are divided into categories according to risk profiles, not all the variation can be explained by the available information. It is also important to note that seasonal factors can be very influential (e.g. adverse weather conditions can contribute to more accidents and therefore more claims). We assume that if each policy has an exposure of one year, that this seasonality is accounted for. It is also accounted for as this applies across all risk categories equally.

```{r tableRatio, fig.cap="Important Ratios/Metrics for Insurance Pricing", paged.print=TRUE}
tab_ratio <- rbind(
  c("Duration", "No. of Claims", "Claim Frequency"),
  c("Duration", "Claims Cost", "Pure Premium"),
  c("No. of Claims", "Claims Cost", "Claim Severity"),
  c("Premium Income", "Claims Cost", "Loss Ratio")
) %>%
  as.data.frame(
    optional = FALSE,
    make.names = TRUE,
  )

colnames(tab_ratio) <- c("Exposure (E)", "Response (Y)", "Ratio (X)")

tab_ratio %>%
  kable(
    format = "latex", booktabs = TRUE,
    caption = "Important Ratios/Metrics for Insurance Pricing",
    row.names = FALSE
  ) %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_styling(latex_options = "striped") %>%
  row_spec(row = 1, bold = TRUE)

```

Generally, these ratios are some outcome divided by the value that measures exposure. For example for claim frequency the exposure is the amount of time that the policyholder is covered for risk, and the outcome is the number of claims they make in that period. This ratio of claim frequency will be the focus of this investigation.

Classical linear regression is the earliest and simplest form of modelling and has been used in a variety of predictive applications due to its ease of interpretation and implementation, it is not however appropriate in the context of insurance pricing as it is not designed to handle the complexities of higher-order applications, such as the representation of non-linear relationships. This is particularly problematic in the context of insurance data, where the outcome is heavily influenced by complex interactions between multiple fields and attributes. Consequently, Generalized Linear Models have emerged since as a significant advancement, offering a more flexible framework that accommodated various distribution families and data types. GLMs excel in modelling discrete and continuous outcomes while retaining interpretability, making them the go-to choice for many years.

Deep Learning models have recently taken industries across the world by storm, not least the insurance industry, ushering in a new era of predictive accuracy. Neural networks, with their multi-layered architecture, can capture intricate patterns in massive datasets, including high-dimensional data, unstructured data, and temporal data. DL models have demonstrated remarkable capabilities in feature extraction, non-linear relationship modelling, and predictive accuracy. They are capable of handling diverse data types such as images, text, and tabular data, which is particularly valuable in the age of big data. These advantages extend to the world of insurance pricing, where their potential remains yet to be completely realised. *TODO: expand!*

The transition from GLMs to Deep Learning is not without its challenges. While DL models excel in predictive performance, they often sacrifice interpretability, which is a crucial requirement in the insurance industry for regulatory compliance and trustworthiness. Additionally, the capabilities of unsupervised learning has been at times overestimated, with many foregoing vital data preprocessing and investigation in the excitement of early demonstrations of these models' performance *TODO: expand!*. Ensuring model fairness *??, expand* and avoiding bias in DL models remains an active area of research, as it is crucial for ethical underwriting and legal compliance. One method to address these concerns is to improve the explainability of these models, or bridge the gap in performance between traditional GLMs and DL models.

\newpage

# Background

## Risk Classification

The process of modelling involves first the collection of data, the subdivision of the data into risk groups using explanatory variables, and finally fitting these explanatory variables to the number of claims and interpreting the results.

Insurance pricing is the task of calculating the optimal price offered by an insurance provider to a potential policyholder. In order to determine the price, it is necessary to determine first the risk posed by the person to be insured. In car insurance statistical models are developed using factors related to the risk. We can distinguish two main approaches

1.  One where individual characteristics are treated as random variables
2.  The other tries to explain variation by observable differences among the insurance profiles/risk groups

Insurance modelling seeks to combine both views.

-   Individual characteristics are variables that describe the policyholder, e.g. age.

-   Vehicle characteristics are variables that describe the car e.g. engine size, brand.

-   Geographical characteristics are variables that describe the environment of the policyholder, e.g. density.

These variables can be either categorical or continuous. However, in using the data to delineate risk *groups*, it can be more helpful when these variables are categorical, and we may endeavour to transform continuous variables into categorical variables through banding for this reason.

Furthermore, certain variables can represent unobservable characteristics. For instance, claim history could be used as a proxy for the evaluation of the quality/skill of the driver. This can be hard to capture statistically, as driving skill would itself have a random effect on the frequency of claims. There are a number of methods, often proprietary and unique to each insurer, to capture this effect. These are referred to as Bonus-Malus Systems.

### Bonus-Malus System

As part of the risk profile of the driver, it is understandably crucial to use their prior history of claims to determine their risk. Insurers use a Bonus-Malus System (BMS) to represent this history. This is often presented to policyholders as some "No-Claims Bonus", where drivers are rewarded with a discounted premium compared to those who make many claims. The dataset used in this investigation is French. In France, it is required by law @service-public that insurers use a Bonus-Malus System, though the particular method that it is applied is up to each insurer.

> "The bonus-malus takes as a reference the period of 12 consecutive months preceding by 2 months the annual expiry of the contract.
>
> > Example :
> >
> > For a contract with an annual maturity of 31 December 2020, the reference period shall be 1^st^ November 2019 to October 31, 2020.
>
> The insurer is obliged to apply the bonus-malus rule. However, the reference premium is determined by the insurer, and each insurer has its own reference premiums. You must therefore check the applicable reference premium in the contract."

### Exposure


## Regression Modelling

~~To understand these models, we will first begin with the simplest form of modelling, namely simple linear regression.~~ We will use this a foundation on which to build and also address the limitations especially in the context of car insurance claims, where in practice heteroskedasticity occurs as we are not dealing with a normal distribution, but rather a Poisson distribution.

Regression modelling assumes a normal distribution, which in turn requires a symmetric density and homoskedastic variance. We will see in Section \@ref(GeneralisedLinearModels) and more demonstrably through the data in Section \@ref(DataAnalysis) how these assumptions do not fit our use case. Our data of claim numbers will be count data, meaning they are bounded by zero, discrete and heteroskedastic. These elements suggest that a Poisson distribution will be more appropriate. We will therefore introduce an expanded family of distributions, known as the Exponential Distribution Family, in Section \@ref(ExponentialDistributionFamily)

We will therefore introduce the Exponential Distribution Family, a more general family of distributions of which the Normal and Poisson are members, to see how this model can be expanded to a more versatile family of models called Generalised Linear Models. This will yield a great deal of information as to the risk associated with an insurance profile. Furthermore, these models will then be again extended and generalised to produce an even more powerful modelling architecture: Neural Networks.

## Classical Linear Regression {#regressionModelling}

```{r figLMMLE, fig.cap = "Linear Model: MLE", out.height="50%"}
# Set seed for reproducibility
set.seed(123)

# Generate nearly linear data
x <- 1:25
y_true <- 2 * x + rnorm(25, mean = 0, sd = 3)
# True linear relationship with normally distributed error

# Fit linear model
model <- lm(y_true ~ x)

# Predictions from the model
y_pred <- predict(model)

# Plot the data and linear model
plot(
  x,
  y_true,
  col = "blue",
  pch = 20,
  main = "Simple Linear Regression",
  xlab = "X",
  ylab = "Y"
)
abline(model, col = "red")

# Add bell curves around each true data
# point to represent normally distributed errors
for (i in seq(1, 25, 5)) {
  points <- rnorm(100, mean = y_pred[i], sd = 1.5)
  d <- density(points)
  lines(i + d$y, d$x, col = "green", lwd = 2)
}

legend(
  "topleft",
  legend = c(
    expression(y[i]),
    expression(
      paste("f(y"[i], " | y"[i], " ~ N(", mu[i], ", ", sigma^2, "))")
    ),
    expression(hat(Y) == beta[0] + beta[1] * X)
  ),
  col = c("blue", "green", "red"),
  pch = c(20, NA, NA),
  lty = c(NA, 1, 1),
  cex = 0.8
)
```

\@ref(fig:figLMMLE) visualises the process of simple linear regression

-   The blue dots are our observations, where the y-axis represents the target variable and the x-axis is the predictor variable

-   The green lines are the density function of normal distributions

-   The red line is our predicted values, which is a straight line through the mean of each normal distribution from which each observation comes.

### Assumptions

Simple Linear Regression works with the following assumptions:

-   The predictor variables $X_1, ... X_p$ are independent
-   The observations $y_i$ are identically independent distributed where $y_i \sim N(\mu_i, \sigma^2)$
-   This $\mu_i$ can be estimated in the form $\mu_i = \beta_0 + \sum^p_{j=1}\beta_jX_{ij}$
-   The $y_is$ have constant variance $\sigma^2$ across all levels, i.e. homeskedasticity

### Estimation

When dealing in one observation, where we have that $y_i \sim N(\mu_i, \sigma^2)$ where $\mu_i = \beta_0 + \sum^p_{j=1}\beta_jX_{ij}$ we can use the probability distribution function of the Normal to evaluate the likelihood of that particular observation emerging from the distribution we have predicted. To do this for all observations we get the joint likelihood or the product of these individual likelihoods. When we consider that our $y_i's$ and $x_i's$ are all known and it is $\beta_0, ..., \beta_p$ that we are estimating, we consider this a function in the form of a Maximum Likelihood Estimator (MLE)

```{=tex}
\begin{align*}
MLE(\beta) & = \min\sum(y_i-\mu_i)^2 \\
MLE(\beta) & = \min\sum(y_i-(\beta_0 + \sum^p_{j=1}\beta_jx_{ij}))^2
\end{align*}
```
## Exponential Distribution Family {#ExponentialDistributionFamily}

Nelder and Wedderburn @nelder_1972_generalized devised a method for generalising the above form, by first generalising the pdf and the notion of a family of distributions. This family takes the form

```{=tex}
\begin{equation} f(y) = exp\left(\frac{y\theta - b(\theta)}{a(\phi)} + c(y, \phi)\right) (\#eq:EDF) \end{equation}
```
Where:

-   $y$ is some observation from a distribution in the EDF

-   $\theta$ is called the natural or canonical parameter, that itself can be expressed as some function of the true mean as $g(\mu)$

-   $\phi$ is called the dispersion parameter

-   $a$, $b$ and $c$ are some known functions of these parameters

It is further shown that by treating the above as a likelihood, and maximising with respect to $\theta$

$$
\frac{\partial\mathcal{L}}{\partial\theta} = y-b'(\theta) := 0
$$

Where $b'(\theta)$ will be equal to the true mean of this distribution, so

$$
\frac{\partial\mathcal{L}}{\partial\theta} = y-\mu = 0
$$

In other words, finding some optimal predictor for $\theta$ will effectively obtain the best estimate for the true mean of the distribution from which $y$ comes.

For the normal distribution we can see that we can take

$$
f(y) = \frac{1}{\sqrt{2\pi\sigma^2}}exp\left({\frac{(y-\mu)^2}{2\sigma^2}}\right)
$$

and rewrite this as

```{=tex}
\begin{align*} 
f(y) & = exp\left( -\frac{(y^2 + \mu^2 - 2y\mu)}{2\sigma_2} -\frac{1}{2}log(2\pi\sigma^2)\right) \\ 
f(y) & = exp\left(\frac{y\mu - \frac{\mu^2}{2}}{\sigma^2} - \frac{y^2}{2\sigma^2} -
\frac{1}{2}log(2\pi\sigma^2)   \right)
\end{align*}
```
And so to obtain \@ref(eq:EDF) we choose the following

```{=tex}
\begin{align*}
\theta & = \mu &
b(\theta) & = \frac{\theta^2}{2}  = \frac{\mu^2}{2} \\
\phi & = \sigma^2 & 
a(\phi) & = \phi  =\sigma^2 \\
c(y,\phi) & = - \frac{y^2}{2\sigma^2} -
\frac{1}{2}log(2\pi\sigma^2)
\end{align*}
```
Here, our natural parameter $\theta$ is simply equal to the mean $\mu$, with $g(\theta)$ simply being the identity function. It is also demonstrated here that

$$
b'(\theta) = \frac{d}{d\theta}\frac{\theta^2}{2} = \theta = \mu
$$

With the combined notions of the MLE and the EDF, we can generalise this process to work for any distribution in the EDF.

# Generalised Linear Models {#GeneralisedLinearModels}

```{r figGLMMLE, fig.cap = "Non-linear Model"}
set.seed(09022024)
x <- 1:25
y_true <- x * rexp(25, rate = 1)
# True linear relationship with normally distributed error

# Fit linear model
model <- lm(y_true ~ x)

# Predictions from the model
y_pred <- predict(model)


# Plot the data and linear model
plot(
  x,
  y_true,
  col = "blue",
  pch = 20,
  xlab = "X",
  ylab = "Y",
  ylim = c(0, 85)
)
abline(model, col = "red")

for (i in seq(2, 25, 4)) {
  points <- rexp(10000, rate = 0.5)
  d <- density(points)
  lines(i + d$y, y_true[i] + d$x, col = "green", lwd = 2)
}

legend(
  "topleft",
  legend = c(
    expression(y[i]),
    expression(paste("f(y"[i], ")")),
    expression(hat(Y) == beta[0] + beta[1] * X)
  ),
  col = c("blue", "green", "red"),
  pch = c(20, NA, NA),
  lty = c(NA, 1, 1),
  cex = 0.8
)
```

Here we see that our $y_is$ are clearly not distributed normally, as the green lines representing the density are skewed, asymmetric, and the variance is not constant across all levels, i.e. the data are heteroskedastic. This means our observations are not distributed normally, rather by some other distribution $f(y_i)$ illustrated by the green densities, and as such any linear predictor, such as is shown in red, will not accurately model our data.

For the purpose of this report, we will be looking at insurance claim as a count, which is known to follow a Poisson distribution, the density of which can be seen below.

```{r figPoiHist, fig.cap = "Histogram of a Poisson Random Variable"}
set.seed(120442916)
y <- rpois(100, 4)

hist(y, freq = FALSE)
lines(density(y), lwd = 2, col = "green")
```

For the poisson distribution, we begin with

$$ f(y) = \frac{e^{-\lambda}\lambda^y}{y!} $$

This can be written as

```{=tex}
\begin{align*} f(y) & = exp\left(-\lambda + y\log(\lambda) -\log(y!) \right) \tag{**} \\ f(y) & = exp\left( \frac{y\log(\lambda) - \lambda}{1} -\log(y_i!)\right)  \end{align*}
```
And so to obtain \@ref(eq:EDF) we choose

```{=tex}
\begin{align*} \theta & = \log(\lambda) & b(\theta) = e^\theta = \lambda \\ \phi & = 1 &  a(\phi) = 1 \\ c(y,\phi) & = -\log(y!) \end{align*}
```
And in this case our natural parameter $\theta$ is equal to the log of our true mean $\lambda$. From this we conclude that to best predict the true mean of the distribution of some $y_i \sim Poi(\lambda_i)$, we now fit a model to this function of $\lambda_i$.

$$ \log(\lambda_i) = \beta_0 + \beta_1x_i $$

Or for multiple $x$

```{=tex}
\begin{equation} \log(\lambda_i) = \beta_0 + \sum^p_{j=1}\beta_jx_{ij} (\#eq:PoiGLM) \end{equation}
```
In the terminology of @nelder_1972_generalized, a GLM is made up of two components: The random component and the systematic component. The random component refers to $\beta_0 + \Sigma^p_{i=1}\beta_ix_i$, that is the linear combination of our predictor variables and our regression parameters, arrived at using the MLE. The random component refers to the $g(\mu_i)$, that is our link function, which captures the fact that our data emerges from a probability distribution of some mean $\mu$ which is dependent on our predictor variables $x_1, \ldots, x_p$ . This yields our general formula for a GLM

```{=tex}
\begin{equation} g(\mu_i) = \beta_0 + \sum^p_{j=1}\beta_jx_{ij} (\#eq:GLM) \end{equation}
```
## Assumptions

Now with the aid of the general form of the EDF, we can expand our regression model to a more generalised from, the GLM. Here the assumptions are as follows

1.  The observations or the $y_is$ are independent

2.  $y_i \sim$ Exponential Dispersion Family, which take the form:

    $$
    f(y) = \exp(\frac{1}{a(\phi)}(y\theta-b(\theta)) + c(y,\phi)
    $$

3.  There is some $g(\mu_i) = \beta_0 + \sum_{j=1}^p \beta_jX_{ij}$. That is we can fit some linear model to a function of the true mean of the probability distribution of each observation.

## Systematic Component

The systematic component refers to the part that is familiar from our simple regression model, that is

$$
\beta_0 + \sum_{j=1}^p \beta_jX_{ij}
$$

Where we have ith observation and

-   $\beta_0, ..., \beta_p$ are our regression parameters

-   We have $p$ predictor variables or risk factors

## Random Component

The random component refers to the left hand side of our equation $g(\mu_i)$. As we have shown in Section \@ref(ExponentialDistributionFamily), this can be shown to be the natural parameter $\theta$ and in our case for claim number counts, which have a Poisson distribution, this will be $\log(\mu_i)$. So for a Poisson GLM we will create a model of the form

$$
\log(\mu_i) = \beta_0 + \sum_{j=1}^p \beta_jX_{ij}
$$

## Estimation

We will once again be using the MLE to determine our regression parameters $\beta_0, ..., \beta_p$, however for a Poisson random variable we get the following MLE:

$$
MLE(\beta|Y) = \sum^n_{i=1} y_ilog(\mu_i) -\mu_i + log(y_i!)
$$

# Neural Networks

Neural Networks often trade off the ease with which GLMs are interpreted for greater predictive accuracy, i.e. calculating the risk more precisely, but yielding little information as to what factors specifically contribute to this risk. It will be discussed that through careful consideration of our NN architecture, one can indeed extract valuable information regarding the relationship between factors and risk.

## Background

### Weights and Biases (Systematic Component)

### Activation Function (Random Component)

### Loss Function (Estimation)

## Shallow Neural Networks (Zero Hidden Layers)

## Neural Networks (One Hidden Layer)

## Deep Neural Networks (Two Hidden Layers)

Neural Networks (NNs), while possibly theorised in the mid 20th century, only began realising their potential in recent decades, as computational resources began to enable the architecture to outperform classical statistical models. As discussed in @lu2020universal, a two layer "deep" NN is sufficient in the context of most problems. The architecture for such networks can be seen below.

```{r figIntroNetwork, results="asis", fig.cap = "Architecture of Neural Network"}
cat(get_network_tex(
  "figIntroNetwork",
  n_input = "p",
  n_hidden1 = "q",
  n_hidden2 = "q"
))
```

```{=tex}
\begin{figure}
  \centering
    \begin{tikzpicture}[>=stealth, node distance=1.5cm, every node/.style={scale=0.8}]
    % Input layer
    \node[circle, draw, fill=yellow!20] (X) at (0,-1) {$X$};
    \node[circle, draw, fill=blue!20] (W1) at (0,-2) {$W_1$};
    \node[circle, draw, fill=blue!20] (b1) at (0,-3) {$b_1$};

    \node[circle, draw, fill=green!20] (star1) at (1,-1) {$*$};
    \node[circle, draw, fill=green!20] (plus1) at (2,-1) {$+$};
    
    \node[circle, draw, fill=red!20] (act_fun1) at (3,-1) {$ Act1 $};

    \node[circle, draw, fill=yellow!20] (h1) at (4,-1) {$H_1$};

    % Labels
    \node[above=0.5cm of X] {Input Layer};

    % Connect Nodes
    \draw[->] (X) -- (star1);
    \draw[->] (W1) -- (star1);

    \draw[->] (star1) -- (plus1);
    \draw[->] (b1) -- (plus1);

    \draw[->] (plus1) -- (act_fun1);
    \draw[->] (act_fun1) -- (h1);

    % First Hidden Layer
    \node[circle, draw, fill=blue!20] (W2) at (4,-2) {$W_2$};
    \node[circle, draw, fill=blue!20] (b2) at (4,-3) {$b_2$};

    \node[circle, draw, fill=green!20] (star2) at (5,-1) {$*$};
    \node[circle, draw, fill=green!20] (plus2) at (6,-1) {$+$};
    
    \node[circle, draw, fill=red!20] (act_fun2) at (7,-1) {$ Act2 $};

    \node[circle, draw, fill=yellow!20] (h2) at (8,-1) {$H_2$};

    % Labels
    \node[above=0.5cm of h1] {Hidden Layer 1};

    % Connect Nodes
    \draw[->] (h1) -- (star2);
    \draw[->] (W2) -- (star2);

    \draw[->] (star2) -- (plus2);
    \draw[->] (b2) -- (plus2);

    \draw[->] (plus2) -- (act_fun2);

    % Second Hidden Layer
    \node[circle, draw, fill=blue!20] (W3) at (8,-2) {$W_3$};
    \node[circle, draw, fill=blue!20] (b3) at (8,-3) {$b_3$};

    \node[circle, draw, fill=green!20] (star3) at (9,-1) {$*$};
    \node[circle, draw, fill=green!20] (plus3) at (10,-1) {$+$};

    \node[circle, draw, fill=red!20] (act_fun3) at (11,-1) {$ Act3 $};

    % Labels
    \node[above=0.5cm of h2] {Hidden Layer 1};

    % Connect Nodes
    \draw[->] (act_fun2) -- (h2);
    \draw[->] (h2) -- (star3);
    \draw[->] (W3) -- (star3);

    \draw[->] (star3) -- (plus3);
    \draw[->] (b3) -- (plus3);

    \draw[->] (plus3) -- (act_fun3);

    % Final Layer
    \node[circle, draw, fill=yellow!20] (output) at (12,-1) {$\hat{y_i}$};
    \node[circle, draw, fill=red!20] (loss) at (12,-2) {$Loss$};
    \node[circle, draw, fill=yellow!20] (truth) at (12,-3) {$y_i$};

    % Labels
    \node[above=0.5cm of act_fun3] {Output};

    % Connect Nodes
    \draw[->] (act_fun3) -- (output);
    \draw[->] (output) -- (loss);
    \draw[->] (truth) -- (loss);

  \end{tikzpicture}
  \caption{Computation Graph of Neural Network}
  \label{fig:figIntroDummyCompGraph}
\end{figure}
```
Figure \ref{fig: figIntroNetwork } and Figure \ref{fig:figIntroDummyCompGraph} are two helpful visualisations of NNs. Figure \ref{fig: figIntroNetwork } shows the fully connected network, with two hidden layers. Figure \ref{fig:figIntroDummyCompGraph} shows the computation graph where:

-   $X$ is a vector of predictor variables

-   The blue nodes represent the model parameters, the weights $W$ and biases $b$, that make up each hidden layer. These are the trainable parameters, equivalent to $\beta_0,...,\beta_p$ in a GLM.

-   The green nodes are simple matrix operations where an incoming arrow indicates input parameters and the outgoing arrows indicate output.

-   The red nodes are our activation functions and also the loss function. The activation functions account for non-linearity in the model, and can be considered similar to how a link function is used in a GLM. The Loss function is used to determine updates to our trainable parameters, similar to how the MLE is used in a GLM.

-   The final two yellow nodes are our prediction $\hat{y_i}$ and the ground truth $y_i$, which are used to calculate the loss function.

It will eventually be seen how we arrive at this model, but firstly, we consider a NN at its simplest. We define a NN with 0 hidden layers, and some activation function $f$. Visualised in a similar fashion to Figure \@ref(fig:figIntroDummyCompGraph) we get

```{=tex}
\begin{figure}
  \centering
    \begin{tikzpicture}[>=stealth, node distance=1.5cm, every node/.style={scale=0.8}]
    % Input layer
    \node[circle, draw, fill=yellow!20] (X) at (0,-1) {$X$};
    \node[circle, draw, fill=blue!20] (W1) at (0,-2) {$W_1$};
    \node[circle, draw, fill=blue!20] (b1) at (0,-3) {$b_1$};

    \node[circle, draw, fill=green!20] (star1) at (1,-1) {$*$};
    \node[circle, draw, fill=green!20] (plus1) at (2,-1) {$+$};
    
    \node[circle, draw, fill=red!20] (act_fun1) at (3,-1) {$ f $};

    % Labels
    \node[above=0.5cm of X] {Input Layer};

    % Connect Nodes
    \draw[->] (X) -- (star1);
    \draw[->] (W1) -- (star1);

    \draw[->] (star1) -- (plus1);
    \draw[->] (b1) -- (plus1);

    \draw[->] (plus1) -- (act_fun1);

    

    % Final Layer
    \node[circle, draw, fill=yellow!20] (output) at (4,-1) {$\hat{y_i}$};
    \node[circle, draw, fill=red!20] (loss) at (4,-2) {$Loss$};
    \node[circle, draw, fill=yellow!20] (truth) at (4,-3) {$y_i$};

    % Labels
    \node[above=0.5cm of output] {Output};

    % Connect Nodes
    \draw[->] (act_fun1) -- (output);
    \draw[->] (output) -- (loss);
    \draw[->] (truth) -- (loss);

  \end{tikzpicture}
  \caption{Computation Graph of Zero-Layer NN}
  \label{fig:figZeroLayerCompGraph}
\end{figure}
```
This is now in a form simple enough to express mathematically as below

```{=tex}
\begin{equation}
\hat{y_i} = f \left( \begin{bmatrix} W_1, \ldots, W_p\end{bmatrix} \begin{bmatrix} x_{i1} \\ \vdots \\ x_{ip} \end{bmatrix} + b \right)
\end{equation}
```
We can multiply out the weights with our predictor variables, and move our activation function to the left-hand side, giving

```{=tex}
\begin{equation} f^{-1}(\hat{y_i}) = b + \sum^p_{j=1}W_jx_{ij} (\#eq:ZeroNN) \end{equation}
```
And recalling our general form of our GLM \@ref(eq:GLM)

```{=tex}
\begin{equation} g(\mu_i) = \beta_0 + \sum^p_{j=1}\beta_jx_{ij} \tag{\ref{eq:GLM}} \end{equation}
```
It can be concluded that our simple NN contains both components of a GLM, namely the random component in the form of $g(\mu_i) = f^{-1}(y_i)$, or the inverse of our activation function, and the systematic component $\beta_0 + \sum^p_{j=1}\beta_jx_{ij} = b + \sum^p_{j=1}W_jx_{ij}$. It remains however to discuss in detail how the trainable parameters in NNs are calculated.

[Loss Function]{.underline}

When dealing in Neural Networks, a loss function is used to evaluate the error in the model iteration, which is then minimised by propagating this error backwards through the network, calculating the gradient of the error with respect to each of the weights, and then updating the weights accordingly.

Many different loss functions are available to choose from. In the context of insurance pricing, we believe that given our data are non-negative, discrete counts and therefore follow a poisson distribution. The loss function in this case is obtained identically to the Maximum Likelihood Estimate for the GLM where we have

$$ \ell(\lambda|y) = \sum^n_{i=1} y_ilog(\lambda_i) -\lambda_i + log(y_i!)$$

In this case where $\lambda$ is the latest model output. However instead of maximising this function we want to minimise it, which is equivalent to maximising the negative of this function.

$Loss = \lambda - ylog(\lambda) - log(y!)$

Thus it can be concluded that the systematic component of a GLM is calculated in the same way as that of the trainable parameters of a NN.

If we recall \@ref(eq:GLM) our GLM formula of

$$\log(\lambda_i) = \beta_0 + \sum^p_{j=1}\beta_jx_{ij}$$

We can rewrite this in matrix notation as

$$ log(\lambda_i) = \beta_0 +  \begin{bmatrix} \beta_1, \ldots, \beta_p\end{bmatrix} \begin{bmatrix} x_{i1} \\ \vdots \\ x_{ip}\end{bmatrix} $$

We can rearrange this as:

$$ log(\lambda_i) = \begin{bmatrix} \beta_1, \ldots, \beta_p\end{bmatrix} \begin{bmatrix} x_{i1} \\ \vdots \\ x_{ip}\end{bmatrix} + \beta_0 $$

And we consider this not as a linear model fitted to a function of some mean, but rather as the exponential of some linear function fitted to the mean we get

```{=tex}
\begin{equation}
\lambda_i = exp \left( \begin{bmatrix} \beta_1, \ldots, \beta_p\end{bmatrix} \begin{bmatrix} x_{i1} \\ \vdots \\ x_{ip} \end{bmatrix} + \beta_0 \right) 
(\#eq:GLMNN)
\end{equation}
```
Now recall our NN formula \@ref(eq:ZeroNN), except with no hidden layers we arrive at

```{=tex}
\begin{equation}
\hat{y} = ActivationFunction \left( \begin{bmatrix} W_1, \ldots, W_p\end{bmatrix} \begin{bmatrix} x_{i1} \\ \vdots \\ x_{ip} \end{bmatrix} + b \right) (\#eq:NNGLM)
\end{equation}
```
We see that designing a NN of 0 hidden layers, with exponential activation function will produce a function \@ref(eq:NNGLM) equivalent to that of a Poisson GLM \@ref(eq:GLMNN). When we also consider that our loss function was defined to be the MLE of a poisson distribution, we see also that the weights and bias of our NN our calculated the same way that a GLM calculates the regression parameters $\beta$.

```{=tex}
\begin{equation} \begin{bmatrix} W_{11} & \ldots & W_{1p} \\                      \vdots & \ddots & \vdots \\                     W_{q1} & \ldots & W_{qp}\end{bmatrix}    \begin{bmatrix} X_1 \\ \vdots \\ X_p\end{bmatrix}  + \begin{bmatrix}b_1 \\ \vdots \\ b_p\end{bmatrix}
 = \begin{bmatrix} H_1 \\ \vdots \\ H_q \end{bmatrix} (\#eq:NNLayer1) \end{equation}
```
# Methodology

```{=html}
<!--
"The data used for this study were collected by..."
-->
```
## Data Collection

This study is carried out using the French motor third partly liability claims frequency data (FreMTPL2freq) available through CASdatasets [@CASdatasets].

```{r tabHead, echo=FALSE, include=TRUE, paged.print=TRUE}
head(cars_orig) %>%
  kable(
    format = "latex", booktabs = TRUE,
    caption = "Structure of the FreMTPL2freq Dataset",
    row.names = FALSE
  ) %>%
  kable_styling(latex_options = "scale_down") %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_styling(latex_options = "striped")
```

The description of the data as outlined in the documentation is as follows:

> "Context: In the dataset freMTPL2freq risk features and claim numbers were collected for 677,991 motor third-part liability policies (observed on a year). Content freMTPL2freq contains 11 columns
>
> -   IDpol: The policy ID (used to link with the claims dataset).
> -   ClaimNb: Number of claims during the exposure period. This is the variable to be predicted
> -   Exposure: The exposure period.
> -   Area: The area code. Six distinct area codes from A to F.
> -   VehPower: The power of the car (ordered categorical).
> -   VehAge: The vehicle age, in years.
> -   DrivAge: The driver age, in years (in France, people can drive a car at 18).
> -   BonusMalus: Bonus/malus, between 50 and 350: 100 means malus in France.
> -   VehBrand: The car brand (unknown categories).
> -   VehGas: The car gas, Diesel or regular.
> -   Density: The density of inhabitants (number of inhabitants per km2) in the city in which the driver of the car lives.
> -   Region: The policy regions in France (based on a standard French classification) "

```{r tableSum, echo=FALSE, include=TRUE, paged.print=TRUE}
cars_orig %>%
  mutate(VehGas = as.factor(VehGas)) %>%
  dplyr::select(-IDpol, -where(is.factor)) %>%
  summary() %>%
  knitr::kable(
    format = "latex", booktabs = TRUE,
    caption = "Summary of Numeric Variables in Data"
  ) %>%
  kable_styling(latex_options = "scale_down") %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_styling(latex_options = "striped")
```

## Data Analysis {#DataAnalysis}

```{r setup_fig, echo=FALSE}
wd <- file.path("C:/Users/patos/Documents/ST4092_120442916")
dsn <- file.path(wd, "data/input/shapefile")
shape <- shapefile(file.path(dsn, "FRA_adm1.shp"))
```

Before investigating the performance of the models, we carry out a number of investigations regarding the structure and behaviour of the data. This is important in identifying which methods to use, issues to address and results to expect.

*Should first step be investigating e.g. cor matrix, i.e. dataset as a whole?*

As ClaimNb is the response variable it is no doubt the most important variable to understand and the first variable we investigate. Figure \@ref(fig:fig_cor)

*Discuss each plot in turn and insights yielded, Motivation behind each plot*

*Histogram of every factor vs ClaimNb - Area, Vehpower, VehAge, VehBrand, VehGas, Region* *Scatterplot of every numeric vs ClaimNb - Exposure, DrivAge, Density* *Map of every variable*

\newpage

### ClaimNb {.unnumbered .unlisted}

```{r figClaimNb, out.width="33%", out.ncol = 3, fig.cap="ClaimNb", fig.subcap = c("ClaimNb by Region", "Histogram of ClaimNb", "Boxplot of ClaimNb")}
create_map(cars_orig, ClaimNb, shape, source_or_run = "run")
hist(cars_orig$ClaimNb, main = "", xlab = "")
boxplot(cars_orig$ClaimNb)
```

### Exposure {.unnumbered .unlisted}

```{r figExposure, out.width = "33%", out.ncol = 3, fig.cap = "Exposure", fig.subcap=c("Exposure by Region", "Histogram of Exposure", "Boxplot of Exposure")}
create_map(cars_orig, Exposure, shape, source_or_run = "run")
hist(cars_orig$Exposure, main = "", xlab = "")
boxplot(cars_orig$Exposure)
```

### Region {.unnumbered .unlisted}

```{r figRegion, out.width = "50%", out.ncol = 2, fig.cap = "Region", fig.subcap = c("ClaimNb by Region", "Exposure by Region")}
boxplot(data = cars_orig, ClaimNb ~ Region)
boxplot(data = cars_orig, Exposure ~ Region)
boxplot(data = cars_orig, Exposure ~ Region)
```

### Area {.unnumbered .unlisted}

```{r figArea, out.width = "50%", out.ncol = 2, fig.cap = "Area", fig.subcap = c("ClaimNb by Area", "Exposure by Area")}
boxplot(data = cars_orig, ClaimNb ~ Area)
boxplot(data = cars_orig, Exposure ~ Area)
```

### VehBrand {.unnumbered .unlisted}

```{r figVehBrand, out.width = "50%", out.ncol = 2, fig.cap = "VehBrand", fig.subcap = c("ClaimNb by VehBrand", "Exposure by VehBrand")}
boxplot(data = cars_orig, ClaimNb ~ VehBrand)
boxplot(data = cars_orig, Exposure ~ VehBrand)
```

### VehPower {.unnumbered .unlisted}

```{r figVehPower, out.width = "33%", out.ncol = 3, fig.cap = "VehPower", fig.subcap=c("VehPower by Region", "Histogram of VehPower", "Boxplot of VehPower")}
create_map(cars_orig, VehPower, shape, source_or_run = "run")
hist(cars_orig$VehPower, main = "", xlab = "")
boxplot(cars_orig$VehPower)

```

### VehAge {.unnumbered .unlisted}

```{r figVehAge, out.width = "33%", out.ncol = 3, fig.cap = "VehAge", fig.subcap=c("VehAge by Region", "Histogram of VehAge", "Boxplot of VehAge")}
create_map(cars_orig, VehAge, shape, source_or_run = "run")
hist(cars_orig$VehAge, main = "", xlab = "")
boxplot(cars_orig$VehAge)
```

### DrivAge {.unnumbered .unlisted}

```{r figDrivAge, out.width = "33%", out.ncol = 3, fig.cap = "DrivAge", fig.subcap=c("DrivAge by Region", "Histogram of DrivAge", "Boxplot of DrivAge")}
create_map(cars_orig, DrivAge, shape, source_or_run = "run")
hist(cars_orig$DrivAge, main = "", xlab = "")
boxplot(cars_orig$DrivAge, main = "")
```

### BonusMalus {.unnumbered .unlisted}

```{r figBonusMalus, out.width = "33%", out.ncol = 3, fig.cap = "BonusMalus", fig.subcap=c("BonusMalus by Region", "Histogram of BonusMalus", "Boxplot of BonusMalus")}
create_map(cars_orig, BonusMalus, shape, source_or_run = "run")
hist(cars_orig$BonusMalus, main = "")
boxplot(cars_orig$BonusMalus)
```

### Density {.unnumbered .unlisted}

```{r figDensity, out.width = "33%", out.ncol = 3, fig.cap = "Density", fig.subcap=c("Density by Region", "Histogram of Density", "Boxplot of Density")}
create_map(cars_orig, Density, shape, source_or_run = "run")
hist(cars_orig$Density, main = "")
boxplot(cars_orig$Density)
```

\newpage

| Variable | Preprocessing Steps |
|----------|---------------------|
| ClaimNb  | Truncated to 5      |
| Exposure | Truncated to 1      |

: List of Preprocessing Steps

<center>

<center>

```{r figCor, include = FALSE, eval = FALSE, fig.cap = "Correlation Heatmap of Features"}
par(mfrow = c(1, 1))
cars_orig %>%
  mutate(across(where(is.factor), ~ as.numeric(unclass(.)))) %>%
  mutate(VehGas = as.numeric(VehGas == "Diesel")) %>%
  dplyr::select(-IDpol) %>%
  cor() %>%
  heatmap(keep.dendro = FALSE)
```

\newpage

</center>

```{r figBox, echo = FALSE, include = TRUE, fig.path = "data/figures/", fig.cap = "Boxplots of Numerical Variables", fig.subcap = c("ClaimNb", "VehPower", "VehAge", "DrivAge", "Exposure"), out.width="20%", fig.ncol=5}
boxplot(cars_orig$ClaimNb, col = "blue")
boxplot(cars_orig$VehPower, col = "green")
boxplot(cars_orig$VehAge, col = "red")
boxplot(cars_orig$DrivAge, col = "orange")
boxplot(cars_orig$Exposure, col = "purple")
```

```{r figHist, echo = FALSE, include = TRUE, fig.path = "data/figures/", fig.cap = "Histograms of Numerical Variables", fig.subcap = c("ClaimNb", "VehPower", "VehAge", "DrivAge", "Exposure"), out.width="20%", fig.ncol=5, out.height="100%"}
ggplot(cars_orig, aes(x = ClaimNb)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black")

ggplot(cars_orig, aes(x = VehPower)) +
  geom_histogram(binwidth = 1, fill = "green", color = "black")

ggplot(cars_orig, aes(x = VehAge)) +
  geom_histogram(binwidth = 1, fill = "red", color = "black")

ggplot(cars_orig, aes(x = DrivAge)) +
  geom_histogram(binwidth = 1, fill = "orange", color = "black")

ggplot(cars_orig, aes(x = Exposure)) +
  geom_histogram(binwidth = 1 / 12, fill = "purple", color = "black")
```

\newpage

## Modelling

### GLMs

#### Data Preprocessing

```{r glmSetup, include=TRUE, echo = FALSE}
# Get dataframe and split
cars_split <- cars_orig %>%
  dplyr::select(-IDpol) %>%
  dplyr::select(where(is.factor), everything()) %>%
  initial_split(prop = 0.8)
train <- training(cars_split)
test <- testing(cars_split)
```

Using the basic R implementation [@stats-3] of GLMs, one can fit a model with the following:

```{r glmFit, include  = TRUE, echo=TRUE}
glm1 <- glm(ClaimNb ~ .,
            data = train,
            family = poisson(link = log),
            offset = Exposure)
```

Yielding:

```{r glmSummaryCoef, include = TRUE, echo = FALSE}
summary(glm1)$coefficients %>%
  as_tibble() %>%
  mutate(
    " " = rownames(summary(glm1)$coefficients),
    Significance = case_when(
      `Pr(>|z|)` == 0 ~ "***",
      `Pr(>|z|)` < 0.001 ~ "**",
      `Pr(>|z|)` < 0.01 ~ "*",
      `Pr(>|z|)` < 0.05 ~ ".",
      .default = " "
    )
  ) %>%
  relocate(" ", .before = everything()) %>%
  kable(
    format = "latex", booktabs = TRUE,
    caption = "Coefficients of fitted GLM",
    row.names = TRUE
  ) %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_styling(latex_options = "striped") %>%
  pack_rows(
    index = c(
      " " = 1,
      "Area" = 5,
      "VehBrand" = 10,
      "Region" = 21,
      "Numeric" = 7
    )
  )
```

```{r glmSumDetails, include = TRUE, echo = FALSE}
cat(
  "     Null Deviance:", summary(glm1)$null.deviance, "on",
  summary(glm1)$df.null, "degrees of freedom\n",
  "Residual Deviance:", summary(glm1)$deviance, "on",
  summary(glm1)$df.residual, "degrees of freedom\n",
  "AIC:", summary(glm1)$aic,
  "\n\n",
  "Number of Fisher Scoring iterations", summary(glm1)$iter
)
```

-   Where: “ClaimNb \~ .”: is the formula, signifying ClaimNb as the response variable and including all other variables as predictors

-   “train” is the training set

-   “family” is defined as poisson using log as the link function

Alternatively for more robust feature selection one can use backwards elimination

```{r glmBackElimEcho, echo = TRUE, eval = FALSE}
backward_elimination <- stats::step(glm1, direction = "backward")
```

```{r glmBackElim, include = TRUE, echo = FALSE}
backward_elimination <- get_backward_elimination(formula = glm1)
```

```{r glmBackSummaryCoef, include = TRUE, echo = FALSE}
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
summary(backward_elimination)$coefficients %>%
  as_tibble() %>%
  mutate(
    " " = rownames(summary(backward_elimination)$coefficients),
    Significance = case_when(
      `Pr(>|z|)` == 0 ~ "***",
      `Pr(>|z|)` < 0.001 ~ "**",
      `Pr(>|z|)` < 0.01 ~ "*",
      `Pr(>|z|)` < 0.05 ~ ".",
      .default = " "
    )
  ) %>%
  relocate(" ", .before = everything()) %>%
  kable(
    format = "latex", booktabs = TRUE,
    caption = "Coefficients of GLM with backwards elimination",
    row.names = TRUE
  ) %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_styling(latex_options = "striped") %>%
  pack_rows(
    index = c(
      " " = 1,
      "Area" = 5,
      "VehBrand" = 10,
      "Region" = 21,
      "Numeric" = 6
    )
  )
```

```{r glmBackElimSumDetails, echo = FALSE, include = TRUE}
cat(
  "     Null Deviance:", summary(backward_elimination)$null.deviance, "on",
  summary(backward_elimination)$df.null, "degrees of freedom\n",
  "Residual Deviance:", summary(backward_elimination)$deviance, "on",
  summary(backward_elimination)$df.residual, "degrees of freedom\n",
  "AIC:", summary(backward_elimination)$aic,
  "\n\n",
  "Number of Fisher Scoring iterations", summary(backward_elimination)$iter
)
```

This is the familiar output of a the summary of a GLM, with the intercept, standard error and significance level of each predictor outlined. It can be seen that the categorical predictors have been split into dummy variables in the cases of Region, VehBrand, and Area. Metrics such as Deviance and AIC for the model as a whole can be seen underneath, alternatively we can evaluate performance with other metrics as seen below:

```{r glmMetrics, echo = FALSE, include=TRUE}
predictions <- predict(glm1, newdata = test, type = "response") %>%
  as_tibble() %>%
  mutate(index = row_number())

val_data <- test %>%
  mutate(index = row_number()) %>%
  left_join(predictions, join_by(index))

yardstick::metrics(data = val_data, truth = ClaimNb, estimate = value) %>%
  knitr::kable(
    format = "latex", booktabs = TRUE,
    caption = "Metrics of GLM Predictions on Validation Data"
  ) %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_styling(latex_options = "striped")
```

We can also look at a selection of predictions made from the resulting model:

```{r glmPredictions, include = TRUE, echo = FALSE, out.width = "100%", paged.print = TRUE}
head(val_data) %>%
  dplyr::select(-index) %>%
  relocate(ClaimNb, .before = value) %>%
  mutate(Prediction = value) %>%
  kable(
    format = "latex", booktabs = TRUE,
    caption = "Predictions from GLM on testing data"
  ) %>%
  kable_styling(latex_options = "scale_down") %>%
  kable_styling(latex_options = "striped") %>%
  kable_styling(latex_options = "HOLD_position") %>%
  column_spec(11:12, bold = TRUE)
```

### Neural Networks

#### Data Preprocessing

One typical problem often faced in training NNs is that of gradient explosion. We aim to prevent this by scaling each of our predictor variables to the range (0,1).

We will be using the keras package [@keras] in R to fit our NNs.

```{r nnSetup, include = TRUE, echo = FALSE}
log_info("Preprocess data")
df_n <- cars_orig %>%
  dplyr::select(where(is.numeric)) %>%
  mutate(across(c(where(is.numeric), -IDpol, -ClaimNb), ~ scale_col(.)))

df <- cars_orig %>%
  mutate(ClaimNb = as.integer(ClaimNb)) %>%
  dplyr::select(c(IDpol, !where(is.numeric))) %>%
  mutate(VehGas = as.integer(VehGas == "Diesel")) %>%
  left_join(df_n, join_by(IDpol)) %>%
  dplyr::select(-IDpol)

## Create Recipe ---------------------------------------------------------------
recipe <- recipe(ClaimNb ~ ., data = df) %>%
  step_dummy(all_factor())

## Bake Data -------------------------------------------------------------------
preprocessed_data <- prep(recipe, training = df, retain = TRUE)
baked_data <- bake(preprocessed_data, new_data = NULL)

## Split Data ------------------------------------------------------------------
log_info("Test/train split")
indices <- sample(seq_len(nrow(df)), 0.8 * nrow(df))
train_data <- baked_data[indices, ]
test_data <- baked_data[-indices, ]

x_train <- train_data %>% dplyr::select(-ClaimNb)
x_test <- test_data %>%
  dplyr::select(-ClaimNb) %>%
  as.matrix()

y_train <- train_data$ClaimNb
y_test <- test_data$ClaimNb
```

```{r nnTune, include = TRUE, echo = FALSE}
# Define tuning grid
t_batchsize <- c(512, 1024)
t_epochs <- c(30, 50)
t_act_final <- c("softplus", "exponential")
t_lr <- c(0.01, 0.001)

t_hidden_nodes <- c(16, 32)
t_hidden_act <- c("relu", "tanh")

tune_grid <- expand.grid(
  batchsize = t_batchsize,
  epochs = t_epochs,
  final_act = t_act_final,
  learn_rate = t_lr,
  t_hidden_act1 = t_hidden_act, t_hidden_nodes1 = t_hidden_nodes,
  t_hidden_act2 = t_hidden_act, t_hidden_nodes2 = t_hidden_nodes
)
```

```{r nnFit, echo = FALSE}
# Get list of fits from tuning
if (getOption("source_or_run") == "source") {
  model_list <- readRDS("data/objects/model_list.rds")
}

# Select best fit based on mse
best_fit <- get_best_fit(model_list, tune_grid)

# Fit best model
if (getOption("source_or_run") == "source") {
  best_model <- readRDS("data/objects/best_model.rds")
} else {
  best_model <- fit_keras_poisson(
    x_train,
    y_train,
    nodes = c(best_fit$t_hidden_nodes1, best_fit$t_hidden_nodes2),
    batchsize = best_fit$batchsize,
    n_epochs = best_fit$epochs,
    act_funs = c(
      best_fit$t_hidden_act1,
      best_fit$t_hidden_act2,
      best_fit$final_act
    ),
    lr = best_fit$learn_rate
  )
}
```

```{r nnMetrics, include = TRUE, echo = FALSE}
# Collect Metrics
metrics_df <- data.frame(
  matrix(rep(numeric(256), 5),
    ncol = 5,
    dimnames = list(
      seq_len(256),
      c("index", "loss", "mse", "val_loss", "val_mse")
    )
  )
) %>%
  head() %>%
  kable(
    format = "latex", booktabs = TRUE,
    caption = "Model Tuning Results",
    row.names = FALSE
  ) %>%
  kable_styling(latex_options = "scale_down") %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_styling(latex_options = "striped")
```

Below we can see the architecture of the neural network

```{r nnFigNetwork, results='asis', echo=FALSE}
cat(get_network_tex(
  name = "figNetwork",
  n_input = best_model$params$shape[1],
  n_hidden1 = best_model$params$shape[2],
  n_hidden2 = best_model$params$shape[3]
))
```

Below we can see the computation graph of the neural network

```{r nnCompGraph, results='asis', echo=FALSE}
cat(get_computation_graph_tex(name = "nnCompGraph",
  act_funs = as.character(c(
    best_fit$t_hidden_act1,
    best_fit$t_hidden_act2,
    "exp"
  ))
))
```

## Results

# Discussion

## Introduction

```{=html}
<!--
o Review findings
o Discuss outcomes
o Stake a claim

"The findings of this study clearly show that..."
-->
```
## Evaluation

```{=html}
<!--
o Analyse
o Offer explanations
o Reference the literature
o State implications

"One explanation for..."
-->
```
## Conclusion

```{=html}
<!--
o Limitations
o Recommendations

"This study was limited by..."
-->
```
# Conclusion

\newpage

# References {.unnumbered}

::: {#refs}
:::

# Acknowledgements {.unnumbered}

# Appendix {.unnumbered}

Simple linear regression is a method of modelling the linear relationship between two random variables, say $X = (x_1,...,x_i,...x_n)$ and $Y = (y_1, ..., y_i, ...y_n)$. We represent this relationship as a line where

$$ \hat{Y} = \beta_0 + \beta_1X \\ $$

Or for each observation

$$ \hat{y_i} = \beta_0 + \beta_1x_i $$

Where $\hat{Y} = (\hat{y_1}, ..., \hat{y_n})$ are our predicted $y_i$ values based on the corresponding $x_i$ and $\beta_0$ is the intercept term, and $\beta_1$ is the slope coefficient.

Above we can see a plot where the blue dots are the observations and the red line is our $\hat{Y} = \beta_0+\beta_1X$. Also included are the green lines, which represent the difference between the prediction and the true value, $y_i-\hat{y_i}$. In order to find the best fitting line, we get the sum of the squares of these differences and minimise this value, yielding the Least Squares Estimate (LSE) of our regression parameters $\beta_0$, $\beta_1$:

$$ LSE(\beta) = \min\sum(y_i-\hat{y}_i)^2 $$

Recalling that

$$\hat{y_i} = \beta_0 + \beta_1x_i$$ We can write this as

```{=tex}
\begin{equation} LSE(\beta) = \min\sum(y_i-(\beta_0+\beta_1x_i))^2 (\#eq:LSE) \end{equation}
```
Now we can find, using partial derivatives, the $\beta_0$ and $\beta_1$ that minimise this value.

This can also be extended for Multiple Linear Regression, where we have more than one predictor variable $x$, and instead we use the formula

Where i = 1,...,n.

However, for the time being, it will be assumed there is only one predictor variable $x$ for ease of notation.

A different approach lies in understanding that often we can assume the following:

1.  The observations $y_i$ are independent

2.  The observations are normally distributed with $y_i \sim N(\mu_i, \sigma^2)$

3.  We can estimate these $\mu_i$ with a line of the form $\mu_i = \beta_0 + \beta_1X_i$

Above is a representation of these assumptions, where we still see some linear relationship between X and Y, where the green lines represent the density of each distribution $N(\mu_i, \sigma^2)$. The key aspect here is we are no longer fitting a line to the observations Y, rather to the means of the normal distributions from which they come. We can then evaluate the likelihood of the ith observation being from the ith distribution with the pdf

$$ \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y_i - \mu_i)^2}{2\sigma^2}} $$

To find the joint probability across all observations we get

$$ \prod^n_{i=1} \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y_i - \mu_i)^2}{2\sigma^2}} $$

But recall that $\mu_i = \beta_0+\beta_1x_i$, and we would like to maximise this probability with respect to the $\beta's$. We can write this as instead a likelihood function

$$ \mathcal{L}(\beta|Y) = \prod^n_{i=1} \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y_i - \mu_i)^2}{2\sigma^2}} $$

We know that any $\beta$ that maximises this function also maximises the log likelihood function denoted $\ell(\beta|Y)$, which also helps in evaluating the right hand side

$$ \ell(\beta|Y) = \sum^n_{i=1} -\log(\sqrt{2\pi\sigma^2}) - \frac{(y_i - \mu_i)^2}{2\sigma^2} $$

Seeing as we are only interested in terms here that relate to $\beta$ and $y_i$, we can ignore some terms and finally we have

$$ \ell(\beta|Y) = -\sum(y_i-\mu_i)^2 $$

We now want to maximise this value, but as it is a negative quantity, this is the same as minimising the negative of the function, and notate this as our Maximum Likelihood Estimator (MLE)

If we replace $\mu_i$ with our linear model, we get

```{=tex}
\begin{equation} MLE(\beta) = \min\sum(y_i-(\beta_0 + \beta_1x_i)^2 (\#eq:MLE) \end{equation}
```
Note: Once again, this form can be extended for multiple predictor variables $x$, where it takes the form

$$ MLE(\beta) = \min\sum^n_{i=1}(y_i-(\beta_0 + \sum^p_{j=1}\beta_jx_{ij}))^2 $$

In the case of a normally distributed random variable, the MLE \@ref(eq:MLE) is identical to the LSE \@ref(eq:LSE). However, for non-normal variables, we require a different framework which first requires addressing the normal distribution deeper, particularly the family of distributions, the Exponential Distribution Family (EDF).
