---
title: "Modelling Architectures Underlying Financial Pricing Problems"

date: "`r Sys.Date()`"

author: "Patrick Reidy"

affiliation: "School of Mathematical Sciences, University College Cork"

email: "120442916@umail.ucc.ie"

output: 
  bookdown::pdf_document2:
    fig_crop: no
    keep_tex: TRUE
    extra_dependencies: "subfig"
  author:
  - name: "Patrick Reidy"
    affiliation: "School of Mathematical Sciences, University College Cork"
    email: "120442916@umail.ucc.ie"
    
header-includes:
  - \usepackage{titling}
  - \pretitle{\begin{center}
    \includegraphics[width=4in,height=2in]{data/figures/University_College_Cork_logo.png}\LARGE\\}
  - \posttitle{\end{center}}

editor_options: 
  chunk_output_type: inline
  
bibliography: references/fyp_references.bib
csl: https://www.zotero.org/styles/ieee
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(CASdatasets)
library(OpenML)
library(farff)
library(tidyverse)
library(tidymodels)
library(stringi)
library(sf)
library(raster)
library(ggplot2)
library(broom)
library(gridExtra)
library(knitr)
library(kableExtra)
library(PerformanceAnalytics)


source("code/functions/create_map.R")

data <- OpenML::getOMLDataSet(data.id = 41214)

cars_orig <- data$data
```

\newpage

\listoffigures

# Abstract

## Problem Statement

## Indication of Methodology

## Main Findings

## Principal Conclusion

# Introduction

The field of insurance pricing has witnessed a significant evolution in modelling techniques over the years, with a transition from traditional regression methods to more sophisticated models such as Generalized Linear Models (GLMs) and, most recently, Deep Learning (DL) models.

Linear regression, as the earliest and simplest form of modelling [@su_2012_linear], has been used in a variety of predictive applications due to its ease of interpretation and implementation. However, it struggled to handle the complexities of higher-order applications, such as the representation of non-linear relationships. This is particularly problematic in the context of insurance data, where the outcome is heavily influenced by complex interactions between multiple fields and attributes. Consequently, Generalized Linear Models emerged as a significant advancement, offering a more flexible framework that accommodated various distribution families and data types. GLMs excel in modelling discrete and continuous outcomes while retaining interpretability, making them the go-to choice for many years.

Deep Learning models have recently taken industries across the world by storm, not least the insurance industry, ushering in a new era of predictive accuracy. Neural networks, with their multilayered architecture, can capture intricate patterns in massive datasets, including high-dimensional data, unstructured data, and temporal data. DL models have demonstrated remarkable capabilities in feature extraction, non-linear relationship modelling, and predictive accuracy. They are capable of handling diverse data types such as images, text, and tabular data, which is particularly valuable in the age of big data. These advantages extend to the world of insurance pricing, where their potential remains yet to be completely realised.*TODO: expand!*

The transition from GLMs to Deep Learning is not without its challenges. While DL models excel in predictive performance, they often sacrifice interpretability, which is a crucial requirement in the insurance industry for regulatory compliance and trustworthiness. Additionally, the capabilities of unsupervised learning has been at times overestimated, with many foregoing vital data preprocessing and investigation in the excitement of early demonstrations of these models' performance*TODO: expand!*. Ensuring model fairness *??, expand* and avoiding bias in DL models remains an active area of research, as it is crucial for ethical underwriting and legal compliance. One method to address these concerns is to improve the explainability of these models, or bridge the gap in performance between traditional GLMs and DL models.

## Background

### Linear Regression

Simple regression models are a statistical method to describe the relationship between two variables. They are made up of two components: the Systematic component, and the random component. In the simplest case they take the form:

$$Y = \beta_0 + \beta_1X + \epsilon$$

Where:

-   $Y$ is the dependent or response variable

-   $X$ is the independent or predictor variable

-   $\beta_0$ is the intercept, representing the value of the response when $X = 0$

-   $\beta_1$ is the coefficient that determines the slope and hence the relationship between $X$ and $Y$

-   $\epsilon$ is the error term or noise, representing unexplained variability in the model

The random component here is the $\epsilon$ term, often assumed to be normally distributed, whereas the systematic component is the $\beta_0 + \beta_1X$

This approach can be expanded to incorporate multiple predictor variables and as such takes the form

$$Y = \beta_0 + (\Sigma_{i=1}^p \beta_iX_i) + \epsilon$$

Where:

-   $Y$, $\beta_0$, and $\epsilon$ are the same as before

-   $p$ is the number of predictor variables

-   $X_i, \ i =(1,...,p)$ are the multiple predictor variables

-   $\beta_i, \ i = (1,...,p)$ are the coefficients that represent the relationship between $Y$ and the $X_i$ 's

*TODO: Expand on each of the above, Underlying assumptions, link between linear regression and GLM*

Underlying assumptions: normally distributed error, least squares estimate

### GLMs

With the advent of Generalised Linear Models discussed by Nelder and Wedderburn[@nelder_1972_generalized], the technique of regression modelling was expanded to yield more robust and effective models. The introduction of a link function is used to capture the non-normal distribution of the data. These are defined as:

$$g(\mu) = \beta_0 + \beta_1X_1 +...+ \beta_pX_p$$

Where:

-   $p$ is the number of predictor variables incorporated into the model

-   $\mu$ is the response variable

-   $g: \mathbb{R} \to \mathbb{R}$ is the link function that relates the expected value of $\mu$ to the linear combination of the predictors

-   $X_1,..., X_p$ are the predictor variables

-   $\beta_1,..., \beta_p$ are the coefficients that determine the slope depending on the values of $X_1,..., X_p$

-   $\beta_0$ represents again the intercept, or the value returned when $X_1,..., X_p$ are all equal to zero

```{=html}
<!--# 
o Establishing a territory
  -> Claiming centrality
  -> Making topic generalisations
  -> Reviewing items of previous research
  
  "In the past decade much research has focused on..."

o Establishing a niche
  -> Counter-claiming
  -> Indicating a gap
  -> Question Raising
  -> Continuing a tradition
  
  "It remains unclear why..."

o Occupying the niche
  -> Outlining purposes
  -> Announcing principal findings
  -> Indicating Structure
  
  "The purpose of this study was to..."
-->
```
# Literature Review

# Methodology

```{=html}
<!--
"The data used for this study were collected by..."
-->
```
## Data Collection

This study is carried out using the French motor third partly liability claims frequency data (FreMTPL2freq) available through CASdatasets [@CASdatasets].

```{r tabHead, echo=FALSE, include=TRUE, paged.print=TRUE}
head(cars_orig) %>%
  kable(
    format = "latex", booktabs = TRUE,
    caption = "Structure of the FreMTPL2freq Dataset",
    row.names = FALSE
  ) %>%
  kable_styling(latex_options = "scale_down") %>%
  kable_styling(latex_options = "HOLD_position") %>% 
  kable_styling(latex_options = "striped")
```

The description of the data as outlined in the documentation is as follows:

*"Context: In the dataset freMTPL2freq risk features and claim numbers were collected for 677,991 motor third-part liability policies (observed on a year). Content freMTPL2freq contains 11 columns*

-   *IDpol: The policy ID (used to link with the claims dataset).*

-   *ClaimNb: Number of claims during the exposure period.* This is the variable to be predicted

-   *Exposure: The exposure period.*

-   *Area: The area code.* Six distinct area codes from A to F.

-   *VehPower: The power of the car (ordered categorical).*

-   *VehAge: The vehicle age, in years.*

-   *DrivAge: The driver age, in years (in France, people can drive a car at 18).*

-   *BonusMalus: Bonus/malus, between 50 and 350: 100 means malus in France.*

-   *VehBrand: The car brand (unknown categories).*

-   *VehGas: The car gas, Diesel or regular.*

-   *Density: The density of inhabitants (number of inhabitants per km2) in the city in which the driver of the car lives.*

-   *Region: The policy regions in France (based on a standard French classification) "*

```{r tableSum, echo=FALSE, include=TRUE, paged.print=TRUE}
cars_orig %>% 
  mutate(VehGas = as.factor(VehGas)) %>% 
  dplyr::select(-IDpol, -where(is.factor)) %>% 
  summary() %>% 
  knitr::kable(format = "latex", booktabs = TRUE,
               caption = "Summary of Variables in Data") %>%
  kable_styling(latex_options = "scale_down") %>%
  kable_styling(latex_options = "HOLD_position") %>% 
  kable_styling(latex_options = "striped")
```

## Data Analysis

```{r setup_fig, echo=FALSE}

wd <- file.path("C:/Users/patos/Documents/ST4092_120442916")
dsn <- file.path(wd, "data/input/shapefile")

shape <- shapefile(file.path(dsn, "FRA_adm1.shp")) 

```

Before investigating the performance of the models, we carry out a number of investigations regarding the structure and behaviour of the data. This is important in identifying which methods to use, issues to address and results to expect.

*Should first step be investigating e.g. cor matrix, i.e. dataset as a whole?*

As ClaimNb is the response variable it is no doubt the most important variable to understand and the first variable we investigate. Figure \@ref(fig:fig_cor)

*Discuss each plot in turn and insights yielded, Motivation behind each plot*

*Histogram of every factor vs ClaimNb - Area, Vehpower, VehAge, VehBrand, VehGas, Region* *Scatterplot of every numeric vs ClaimNb - Exposure, DrivAge, Density* *Map of every variable*

See Figure \@ref(fig:figHist-1)

See Figure \@ref(fig:figHist-2)

See Figure \@ref(fig:figHist-3)

See Figure \@ref(fig:figHist-4)

\newpage

<center>

<center>

```{r figCor, echo=FALSE, fig.cap = "Correlation Heatmap of Features", fig.path = "data/figures/"}
par(mfrow = c(1, 1))
cars_numeric <- cars_orig %>%
  mutate(across(where(is.factor), ~ as.numeric(unclass(.)))) %>%
  mutate(VehGas = as.numeric(VehGas == "Diesel")) %>%
  dplyr::select(-IDpol)

cor(cars_numeric) %>%
  heatmap(keep.dendro = FALSE)
```

\newpage

</center>

```{r figBox, echo = FALSE, include = TRUE, fig.path = "data/figures/", fig.cap = "Boxplots of Numerical Variables", fig.subcap = c("ClaimNb", "VehPower", "VehAge", "DrivAge", "Exposure"), out.width="20%", fig.ncol=5}

# 
# ggplot(cars_orig, aes(x = ClaimNb)) +
#   geom_boxplot(fill = "blue", color = "black", notch = TRUE, notch_width = 0.2) +
#   coord_flip()
# 
# ggplot(cars_orig, aes(x = VehPower)) +
#   geom_boxplot(fill = "green", color = "black", notch = TRUE, notchwidth = 0.2) +
#   coord_flip()
# 
# ggplot(cars_orig, aes(x = VehAge)) +
#   geom_boxplot(fill = "red", color = "black") +
#   stat_boxplot(geom='errorbar', width = 0.2) +
#   coord_flip()
# 
# ggplot(cars_orig, aes(x = DrivAge)) +
#   geom_boxplot(fill = "orange", color = "black") +
#   stat_boxplot(geom='errorbar') +
#   coord_flip()
# 
# ggplot(cars_orig, aes(x = Exposure)) +
#   geom_boxplot(fill = "purple", color = "black") +
#   stat_boxplot(geom='errorbar') +
#   coord_flip()

boxplot(cars_orig$ClaimNb, col = "blue")
boxplot(cars_orig$VehPower, col = "green")
boxplot(cars_orig$VehAge, col = "red")
boxplot(cars_orig$DrivAge, col = "orange")
boxplot(cars_orig$Exposure, col = "purple")

```

```{r figHist, echo = FALSE, include = TRUE, fig.path = "data/figures/", fig.cap = "Histograms of Numerical Variables", fig.subcap = c("ClaimNb", "VehPower", "VehAge", "DrivAge", "Exposure"), out.width="20%", fig.ncol=5, out.height="100%"}

ggplot(cars_orig, aes(x = ClaimNb)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") 

ggplot(cars_orig, aes(x = VehPower)) +
  geom_histogram(binwidth = 1, fill = "green", color = "black") 

ggplot(cars_orig, aes(x = VehAge)) +
  geom_histogram(binwidth = 1, fill = "red", color = "black") 

ggplot(cars_orig, aes(x = DrivAge)) +
  geom_histogram(binwidth = 1, fill = "orange", color = "black")

ggplot(cars_orig, aes(x = Exposure)) +
  geom_histogram(binwidth = 1/12, fill = "purple", color = "black")
```

\newpage

## Modelling

### GLMs

```{r glmSetup, include=TRUE, echo = FALSE}
# Get dataframe
cars_glm <- cars_orig %>% 
  dplyr::select(-IDpol) %>% 
  dplyr::select(where(is.factor), everything())

# Split data
cars_split <- initial_split(cars_glm, prop = 0.8)
train <- training(cars_split)
test <- testing(cars_split)
```

Using the basic stats \@[@stats-3] implementation of GLMs, one can fit a model with the following:

```{r glmFit, include  = TRUE, echo=TRUE}
glm1 <- glm(ClaimNb ~ ., data = train, family = poisson(link=log), offset = Exposure)
```

Yielding:

```{r glmSummaryCoef, include = TRUE, echo = FALSE}
glmSum <- summary(glm1)

glmSum$coefficients %>% 
  as_tibble() %>% 
  mutate(
    " " = rownames(glmSum$coefficients),
    Significance = case_when(
    `Pr(>|z|)` == 0 ~ "***",
    `Pr(>|z|)` < 0.001 ~ "**",
    `Pr(>|z|)` < 0.01 ~ "*",
    `Pr(>|z|)` < 0.05 ~ ".",
    .default = " "    
  )) %>% 
  relocate(" ", .before = everything()) %>% 
  kable(
    format = "latex", booktabs = TRUE,
    caption = "Coefficients of fitted GLM",
    row.names = TRUE
  ) %>%
  kable_styling(latex_options = "HOLD_position") %>% 
  kable_styling(latex_options = "striped") %>% 
  pack_rows(
    index = c(
      " " = 1,
      "Area" = 5, 
      "VehBrand" = 10, 
      "Region" = 21, 
      "Numeric" = 7
    )
  )


```

```{r glmSumDetails, include = TRUE, echo = FALSE}

cat(
  "     Null Deviance:", glmSum$null.deviance,  "on", glmSum$df.null, "degrees of freedom\n",
  "Residual Deviance:", glmSum$deviance, "on", glmSum$df.residual, "degrees of freedom\n",
  "AIC:", glmSum$aic,
  "\n\n",
  "Number of Fisher Scoring iterations", glmSum$iter
  )
```

Where: • “ClaimNb \~ .”: is the formula, signifying ClaimNb as the response variable and including all other variables as predictors. • “train” is the training set • “family” is defined as poisson using log as the link function.

Alternatively for more robust feature selection one can use backwards elimination

```{r glmBackElim, include = TRUE, echo = TRUE}
backward_elimination <- stats::step(glm1, direction = "backward", trace = 0)
```

```{r glmBackSummaryCoef, include = TRUE, echo = FALSE}
glmBackElimSum <- summary(backward_elimination)

# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

glmBackElimSum$coefficients %>% 
  as_tibble() %>% 
  mutate(
    " " = rownames(glmBackElimSum$coefficients),
    Significance = case_when(
    `Pr(>|z|)` == 0 ~ "***",
    `Pr(>|z|)` < 0.001 ~ "**",
    `Pr(>|z|)` < 0.01 ~ "*",
    `Pr(>|z|)` < 0.05 ~ ".",
    .default = " "    
  )) %>% 
  relocate(" ", .before = everything()) %>% 
  kable(
    format = "latex", booktabs = TRUE,
    caption = "Coefficients of GLM with backwards elimination",
    row.names = TRUE
  ) %>%
  kable_styling(latex_options = "HOLD_position") %>% 
  kable_styling(latex_options = "striped") %>% 
  pack_rows(
    index = c(
      " " = 1,
      "Area" = 5, 
      "VehBrand" = 10, 
      "Region" = 21, 
      "Numeric" = 6
    )
  )
```

```{r glmBackElimSumDetails, echo = FALSE, include = TRUE}
cat(
  "     Null Deviance:", glmBackElimSum$null.deviance,  "on", glmBackElimSum$df.null, "degrees of freedom\n",
  "Residual Deviance:", glmBackElimSum$deviance, "on", glmBackElimSum$df.residual, "degrees of freedom\n",
  "AIC:", glmBackElimSum$aic,
  "\n\n",
  "Number of Fisher Scoring iterations", glmBackElimSum$iter
  )

```

This is the familiar output of a the summary of a GLM, with the intercept, standard error and significance level of each predictor outlined. It can be seen the categorical predictors have been split into dummy variables in the cases of Region, VehBrand, and Area. Metrics for the model as a whole can be seen underneath, alternatively we can evaluate performance with other metrics as seen below:

```{r glmMetrics, echo = FALSE, include=TRUE}
predictions <- predict(glm1, newdata = test, type = "response") %>% 
  as_tibble() %>% 
  mutate(index = seq(nrow(test)))


val_data <- test %>% 
  mutate(index = seq(nrow(test))) %>% 
  left_join(predictions, join_by(index))

yardstick::metrics(data = val_data, truth = ClaimNb, estimate = value) %>% 
  knitr::kable(format = "latex", booktabs = TRUE,
    caption = "Metrics of GLM Predictions on Validation Data") %>% 
  kable_styling(latex_options = "HOLD_position") %>% 
  kable_styling(latex_options = "striped") 

```

We can also look at a selection of predictions made from the resulting model:

```{r glmPredictions, include = TRUE, echo = FALSE, out.width = "100%", paged.print = TRUE}

head(val_data) %>% 
  dplyr::select(-index) %>% 
  relocate(ClaimNb, .before = value) %>% 
  mutate(Prediction = exp(value)) %>% 
  kable(format = "latex", booktabs = TRUE,
    caption = "Predictions from GLM on testing data") %>% 
  kable_styling(latex_options = "scale_down") %>% 
  kable_styling(latex_options = "striped") %>% 
  kable_styling(latex_options = "HOLD_position") %>%
  column_spec(11:12, color = "gray", bold  = TRUE) 
```

## Results

# Discussion

## Introduction

```{=html}
<!--
o Review findings
o Discuss outcomes
o Stake a claim

"The findings of this study clearly show that..."
-->
```
## Evaluation

```{=html}
<!--
o Analyse
o Offer explanations
o Reference the literature
o State implications

"One explanation for..."
-->
```
## Conclusion

```{=html}
<!--
o Limitations
o Recommendations

"This study was limited by..."
-->
```
# Conclusion

# Acknowledgements

# References

::: {#refs}
:::
